{
  "Host": "sagemaker",
  "Model Id": "NousResearch/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.8xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 46,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 30.198866359527393,
  "Latency (ms/token) avg": 33.109896543478264,
  "Latency (ms/token) min": 32.893752,
  "Latency (ms/token) med": 33.150863,
  "Latency (ms/token) max": 33.233572,
  "Latency (ms/token) p(90)": 33.198219,
  "Latency (ms/token) p(95)": 33.22701075,
  "Latency Request ms p(90)": 1660.1186475,
  "Latency Request ms p(95)": 1661.647082,
  "Latency Request ms avg": 1655.691290021739,
  "Latency Request ms min": 1644.9014319999999,
  "Latency Request ms med": 1657.748721,
  "Latency Request ms max": 1661.843537,
  "Latency Inference ms med": 1657.5431875,
  "Latency Inference ms max": 1661.678644,
  "Latency Inference ms p(90)": 1659.9109815000002,
  "Latency Inference ms p(95)": 1661.350555,
  "Latency Inference ms avg": 1655.494855326087,
  "Latency Inference ms min": 1644.687639,
  "Queue time ms med": 0.0356055,
  "Queue time ms max": 0.065621,
  "Queue time ms p(90)": 0.04215,
  "Queue time ms p(95)": 0.044281,
  "Queue time ms avg": 0.036156913043478255,
  "Queue time ms min": 0.02524
}