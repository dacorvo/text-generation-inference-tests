{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 50,
  "Virtual Users": 1,
  "Duration (s)": 181,
  "Throughput (tokens/second)": 137.67833314434242,
  "Latency (ms/token) avg": 6.82371544,
  "Latency (ms/token) min": 6.649594,
  "Latency (ms/token) med": 6.814088,
  "Latency (ms/token) max": 7.303559,
  "Latency (ms/token) p(90)": 6.9219002,
  "Latency (ms/token) p(95)": 6.94552095,
  "Latency Request ms p(90)": 3461.3505733999996,
  "Latency Request ms p(95)": 3473.1543001,
  "Latency Request ms avg": 3412.2300659199996,
  "Latency Request ms min": 3325.209673,
  "Latency Request ms med": 3407.3636175,
  "Latency Request ms max": 3652.248495,
  "Latency Inference ms med": 3407.0442165,
  "Latency Inference ms max": 3651.779546,
  "Latency Inference ms p(90)": 3460.9503126000004,
  "Latency Inference ms p(95)": 3472.7606441,
  "Latency Inference ms avg": 3411.8579817400005,
  "Latency Inference ms min": 3324.797415,
  "Queue time ms med": 0.0650665,
  "Queue time ms max": 0.09961199999999999,
  "Queue time ms p(90)": 0.080233,
  "Queue time ms p(95)": 0.08193204999999999,
  "Queue time ms avg": 0.057909459999999996,
  "Queue time ms min": 0.017839999999999998
}