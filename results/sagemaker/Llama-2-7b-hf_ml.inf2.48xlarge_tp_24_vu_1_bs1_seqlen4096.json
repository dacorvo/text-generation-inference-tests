{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 41,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 118.30583276196698,
  "Latency (ms/token) avg": 8.451122268292682,
  "Latency (ms/token) min": 7.570025,
  "Latency (ms/token) med": 8.532929,
  "Latency (ms/token) max": 9.984921,
  "Latency (ms/token) p(90)": 8.815313,
  "Latency (ms/token) p(95)": 8.945421,
  "Latency Request ms p(90)": 4408.504352,
  "Latency Request ms p(95)": 4473.0358160000005,
  "Latency Request ms avg": 4226.334309365854,
  "Latency Request ms min": 3785.897748,
  "Latency Request ms med": 4267.038197,
  "Latency Request ms max": 4993.186164,
  "Latency Inference ms med": 4266.464908,
  "Latency Inference ms max": 4992.460606,
  "Latency Inference ms p(90)": 4407.656938,
  "Latency Inference ms p(95)": 4472.71066,
  "Latency Inference ms avg": 4225.56137804878,
  "Latency Inference ms min": 3785.012682,
  "Queue time ms med": 0.022251,
  "Queue time ms max": 0.289764,
  "Queue time ms p(90)": 0.030461,
  "Queue time ms p(95)": 0.039622,
  "Queue time ms avg": 0.030911536585365856,
  "Queue time ms min": 0.015551
}