{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 93,
  "Virtual Users": 1,
  "Duration (s)": 303,
  "Throughput (tokens/second)": 153.40841887432617,
  "Latency (ms/token) avg": 6.492326247311828,
  "Latency (ms/token) min": 6.425028,
  "Latency (ms/token) med": 6.48724,
  "Latency (ms/token) max": 6.961101,
  "Latency (ms/token) p(90)": 6.5383998,
  "Latency (ms/token) p(95)": 6.556963199999999,
  "Latency Request ms p(90)": 3269.4394153999997,
  "Latency Request ms p(95)": 3278.7327585999997,
  "Latency Request ms avg": 3246.4005443333335,
  "Latency Request ms min": 3212.8623159999997,
  "Latency Request ms med": 3243.8701349999997,
  "Latency Request ms max": 3480.846475,
  "Latency Inference ms med": 3243.620313,
  "Latency Inference ms max": 3480.550601,
  "Latency Inference ms p(90)": 3269.2001357999998,
  "Latency Inference ms p(95)": 3278.481829,
  "Latency Inference ms avg": 3246.1633872795696,
  "Latency Inference ms min": 3212.5144090000003,
  "Queue time ms med": 0.032252,
  "Queue time ms max": 0.063873,
  "Queue time ms p(90)": 0.042546,
  "Queue time ms p(95)": 0.046365999999999984,
  "Queue time ms avg": 0.03299820430107527,
  "Queue time ms min": 0.016331
}