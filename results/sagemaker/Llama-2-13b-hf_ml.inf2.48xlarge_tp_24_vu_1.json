{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 26,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 70.09430402941672,
  "Latency (ms/token) avg": 14.265245192307694,
  "Latency (ms/token) min": 12.65587,
  "Latency (ms/token) med": 14.297706999999999,
  "Latency (ms/token) max": 15.22584,
  "Latency (ms/token) p(90)": 15.0185855,
  "Latency (ms/token) p(95)": 15.069075250000001,
  "Latency Request ms p(90)": 7510.02644,
  "Latency Request ms p(95)": 7535.0700925,
  "Latency Request ms avg": 7133.247229192308,
  "Latency Request ms min": 6328.533588,
  "Latency Request ms med": 7149.489029,
  "Latency Request ms max": 7613.867679,
  "Latency Inference ms med": 7148.853803,
  "Latency Inference ms max": 7612.920193,
  "Latency Inference ms p(90)": 7509.293041999999,
  "Latency Inference ms p(95)": 7534.53795275,
  "Latency Inference ms avg": 7132.622831961538,
  "Latency Inference ms min": 6327.935176999999,
  "Queue time ms med": 0.019805500000000004,
  "Queue time ms max": 0.07104099999999999,
  "Queue time ms p(90)": 0.052606,
  "Queue time ms p(95)": 0.0575485,
  "Queue time ms avg": 0.029458653846153846,
  "Queue time ms min": 0.01676
}