{"Host": "sagemaker", "Model Id": "TheBloke/Llama-2-7B-GPTQ", "Instance": "ml.g5.2xlarge", "Tensor parallelism degree": 1, "quantization": "gptq", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 299, "Virtual Users": 10, "Throughput (tokens/second)": 167.5778881856736, "Latency (ms/token) avg": 56.0147747424749, "Latency (ms/token) min": 37.630411, "Latency (ms/token) med": 52.730564, "Latency (ms/token) max": 462.916519, "Latency (ms/token) p(90)": 62.506295200000004, "Latency (ms/token) p(95)": 71.11783079999998, "Latency Request ms p(90)": 3565.7902102000003, "Latency Request ms p(95)": 4116.4442271, "Latency Request ms avg": 2983.6872001036795, "Latency Request ms min": 427.663885, "Latency Request ms med": 2761.30017, "Latency Request ms max": 7758.388550000001, "Latency Inference ms med": 2618.633949, "Latency Inference ms max": 7757.987071, "Latency Inference ms p(90)": 3084.4612752, "Latency Inference ms p(95)": 3290.5967581, "Latency Inference ms avg": 2663.5161316755857, "Latency Inference ms min": 277.151897, "Queue time ms med": 148.444237, "Queue time ms max": 3683.509704, "Queue time ms p(90)": 850.0639032, "Queue time ms p(95)": 1208.6163951, "Queue time ms avg": 318.95190160200667, "Queue time ms min": 0.046851}