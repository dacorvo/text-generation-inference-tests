{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 9,
  "Virtual Users": 1,
  "Duration (s)": 200,
  "Throughput (tokens/second)": 22.46426679057814,
  "Latency (ms/token) avg": 44.47872022222222,
  "Latency (ms/token) min": 44.235817,
  "Latency (ms/token) med": 44.333742,
  "Latency (ms/token) max": 45.567119,
  "Latency (ms/token) p(90)": 44.7255966,
  "Latency (ms/token) p(95)": 45.1463578,
  "Latency Request ms p(90)": 22363.1459616,
  "Latency Request ms p(95)": 22573.598892799997,
  "Latency Request ms avg": 22239.67443911111,
  "Latency Request ms min": 22118.268993,
  "Latency Request ms med": 22167.119001,
  "Latency Request ms max": 22784.051824,
  "Latency Inference ms med": 22166.871286999998,
  "Latency Inference ms max": 22783.559555,
  "Latency Inference ms p(90)": 22362.7984774,
  "Latency Inference ms p(95)": 22573.1790162,
  "Latency Inference ms avg": 22239.360306222217,
  "Latency Inference ms min": 22117.908561,
  "Queue time ms med": 0.035232,
  "Queue time ms max": 0.049783,
  "Queue time ms p(90)": 0.048511,
  "Queue time ms p(95)": 0.049147,
  "Queue time ms avg": 0.03760688888888889,
  "Queue time ms min": 0.025652
}