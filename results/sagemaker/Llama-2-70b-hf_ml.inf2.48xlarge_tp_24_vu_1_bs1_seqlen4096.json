{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 8,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 22.525033373280348,
  "Latency (ms/token) avg": 44.3944805,
  "Latency (ms/token) min": 44.181437,
  "Latency (ms/token) med": 44.2640155,
  "Latency (ms/token) max": 45.35182,
  "Latency (ms/token) p(90)": 44.637434299999995,
  "Latency (ms/token) p(95)": 44.99462715,
  "Latency Request ms p(90)": 22319.0667256,
  "Latency Request ms p(95)": 22497.6789468,
  "Latency Request ms avg": 22197.5253805,
  "Latency Request ms min": 22090.946193,
  "Latency Request ms med": 22132.3233915,
  "Latency Request ms max": 22676.291168,
  "Latency Inference ms med": 22132.008011500002,
  "Latency Inference ms max": 22675.910062,
  "Latency Inference ms p(90)": 22318.7173793,
  "Latency Inference ms p(95)": 22497.31372065,
  "Latency Inference ms avg": 22197.240506125,
  "Latency Inference ms min": 22090.718939,
  "Queue time ms med": 0.029726000000000002,
  "Queue time ms max": 0.047810000000000005,
  "Queue time ms p(90)": 0.0459137,
  "Queue time ms p(95)": 0.046861850000000004,
  "Queue time ms avg": 0.032461875,
  "Queue time ms min": 0.022321
}