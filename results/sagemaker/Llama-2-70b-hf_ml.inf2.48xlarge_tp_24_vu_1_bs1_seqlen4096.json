{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 14,
  "Virtual Users": 1,
  "Duration (s)": 311,
  "Throughput (tokens/second)": 22.47337338566122,
  "Latency (ms/token) avg": 44.45894435714286,
  "Latency (ms/token) min": 44.234792,
  "Latency (ms/token) med": 44.4138965,
  "Latency (ms/token) max": 45.449909,
  "Latency (ms/token) p(90)": 44.4784576,
  "Latency (ms/token) p(95)": 44.8215891,
  "Latency Request ms p(90)": 22239.4728933,
  "Latency Request ms p(95)": 22411.12186725,
  "Latency Request ms avg": 22229.770558071432,
  "Latency Request ms min": 22117.722422,
  "Latency Request ms med": 22207.272378499998,
  "Latency Request ms max": 22725.396435,
  "Latency Inference ms med": 22206.948439,
  "Latency Inference ms max": 22724.954729,
  "Latency Inference ms p(90)": 22239.2289003,
  "Latency Inference ms p(95)": 22410.794673700002,
  "Latency Inference ms avg": 22229.472362642857,
  "Latency Inference ms min": 22117.396128,
  "Queue time ms med": 0.033511,
  "Queue time ms max": 0.089531,
  "Queue time ms p(90)": 0.04844800000000001,
  "Queue time ms p(95)": 0.06470684999999998,
  "Queue time ms avg": 0.03843857142857143,
  "Queue time ms min": 0.02582
}