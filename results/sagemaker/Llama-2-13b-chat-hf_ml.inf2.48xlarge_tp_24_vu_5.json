{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 173,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 95.09609329356402,
  "Latency (ms/token) avg": 37.14911428323699,
  "Latency (ms/token) min": 20.97982,
  "Latency (ms/token) med": 35.535201,
  "Latency (ms/token) max": 43.455418,
  "Latency (ms/token) p(90)": 42.394458,
  "Latency (ms/token) p(95)": 42.5260756,
  "Latency Request ms p(90)": 4226.1850998,
  "Latency Request ms p(95)": 4237.0503096,
  "Latency Request ms avg": 2628.9197730578035,
  "Latency Request ms min": 2068.104145,
  "Latency Request ms med": 2110.070636,
  "Latency Request ms max": 4297.900587,
  "Latency Inference ms med": 1776.760071,
  "Latency Inference ms max": 2172.7709339999997,
  "Latency Inference ms p(90)": 2119.7229322,
  "Latency Inference ms p(95)": 2126.3038030000002,
  "Latency Inference ms avg": 1857.4557366473991,
  "Latency Inference ms min": 1048.9910149999998,
  "Queue time ms med": 334.930525,
  "Queue time ms max": 2163.2996740000003,
  "Queue time ms p(90)": 2107.757235,
  "Queue time ms p(95)": 2111.2622584,
  "Queue time ms avg": 771.2131293121387,
  "Queue time ms min": 0.032170000000000004
}