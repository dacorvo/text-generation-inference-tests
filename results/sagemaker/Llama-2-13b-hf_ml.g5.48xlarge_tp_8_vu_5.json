{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-13b-hf", "Instance": "ml.g5.48xlarge", "Tensor parallelism degree": 8, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 242, "Virtual Users": 5, "Throughput (tokens/second)": 139.18820292268381, "Latency (ms/token) avg": 44.425146392561984, "Latency (ms/token) min": 21.426108, "Latency (ms/token) med": 35.055391, "Latency (ms/token) max": 605.188296, "Latency (ms/token) p(90)": 50.5603029, "Latency (ms/token) p(95)": 55.79932234999997, "Latency Request ms p(90)": 2466.3770558, "Latency Request ms p(95)": 2538.78882765, "Latency Request ms avg": 1796.1292318636363, "Latency Request ms min": 169.300645, "Latency Request ms med": 1722.1974985, "Latency Request ms max": 3140.815641, "Latency Inference ms med": 1682.374248, "Latency Inference ms max": 3059.37801, "Latency Inference ms p(90)": 2434.7529498999997, "Latency Inference ms p(95)": 2528.30119045, "Latency Inference ms avg": 1738.2712712933885, "Latency Inference ms min": 114.675364, "Queue time ms med": 12.386136, "Queue time ms max": 738.939713, "Queue time ms p(90)": 162.3889196000001, "Queue time ms p(95)": 261.4134709, "Queue time ms avg": 56.878499805785125, "Queue time ms min": 0.026171}