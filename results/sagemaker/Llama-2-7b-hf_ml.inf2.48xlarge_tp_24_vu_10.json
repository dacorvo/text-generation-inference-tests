{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 122,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 313.97038416428677,
  "Latency (ms/token) avg": 24.818852827868852,
  "Latency (ms/token) min": 20.887596,
  "Latency (ms/token) med": 24.6503855,
  "Latency (ms/token) max": 25.970187,
  "Latency (ms/token) p(90)": 25.713480999999998,
  "Latency (ms/token) p(95)": 25.7650546,
  "Latency Request ms p(90)": 25670.6594229,
  "Latency Request ms p(95)": 25714.25096065,
  "Latency Request ms avg": 15925.06889880328,
  "Latency Request ms min": 12672.518887,
  "Latency Request ms med": 12842.855220000001,
  "Latency Request ms max": 25830.839211000002,
  "Latency Inference ms med": 12325.193018,
  "Latency Inference ms max": 12985.093918,
  "Latency Inference ms p(90)": 12856.7407765,
  "Latency Inference ms p(95)": 12882.5274684,
  "Latency Inference ms avg": 12409.42664504918,
  "Latency Inference ms min": 10443.798071,
  "Queue time ms med": 516.7078309999999,
  "Queue time ms max": 12935.742755,
  "Queue time ms p(90)": 12829.428386200001,
  "Queue time ms p(95)": 12853.27252345,
  "Queue time ms avg": 3515.4619181639346,
  "Queue time ms min": 0.069881
}