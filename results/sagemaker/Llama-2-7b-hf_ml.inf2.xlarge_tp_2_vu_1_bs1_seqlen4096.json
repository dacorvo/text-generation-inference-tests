{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 10,
  "Virtual Users": 1,
  "Duration (s)": 183,
  "Throughput (tokens/second)": 27.29869499433529,
  "Latency (ms/token) avg": 33.27751919999999,
  "Latency (ms/token) min": 33.218006,
  "Latency (ms/token) med": 33.2451785,
  "Latency (ms/token) max": 33.539097,
  "Latency (ms/token) p(90)": 33.3083181,
  "Latency (ms/token) p(95)": 33.42370755,
  "Latency Request ms p(90)": 16654.3622987,
  "Latency Request ms p(95)": 16712.11524785,
  "Latency Request ms avg": 16639.122964599996,
  "Latency Request ms min": 16609.194669,
  "Latency Request ms med": 16623.4876055,
  "Latency Request ms max": 16769.868197,
  "Latency Inference ms med": 16622.589640500002,
  "Latency Inference ms max": 16769.548695,
  "Latency Inference ms p(90)": 16654.159077599998,
  "Latency Inference ms p(95)": 16711.8538863,
  "Latency Inference ms avg": 16638.7597925,
  "Latency Inference ms min": 16609.003078,
  "Queue time ms med": 0.0392555,
  "Queue time ms max": 0.046411,
  "Queue time ms p(90)": 0.043692999999999996,
  "Queue time ms p(95)": 0.045051999999999995,
  "Queue time ms avg": 0.0387413,
  "Queue time ms min": 0.03107
}