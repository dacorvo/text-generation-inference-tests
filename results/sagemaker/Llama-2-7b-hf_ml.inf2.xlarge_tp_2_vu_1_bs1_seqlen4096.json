{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 9,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 30.210409798912835,
  "Latency (ms/token) avg": 33.100254666666665,
  "Latency (ms/token) min": 33.063318,
  "Latency (ms/token) med": 33.068674,
  "Latency (ms/token) max": 33.358112,
  "Latency (ms/token) p(90)": 33.1301488,
  "Latency (ms/token) p(95)": 33.244130399999996,
  "Latency Request ms p(90)": 16566.9799752,
  "Latency Request ms p(95)": 16623.1797396,
  "Latency Request ms avg": 16550.58648088889,
  "Latency Request ms min": 16531.872021000003,
  "Latency Request ms med": 16534.536838,
  "Latency Request ms max": 16679.379504,
  "Latency Inference ms med": 16534.337204,
  "Latency Inference ms max": 16679.056128,
  "Latency Inference ms p(90)": 16565.074514400003,
  "Latency Inference ms p(95)": 16622.0653212,
  "Latency Inference ms avg": 16550.127559222223,
  "Latency Inference ms min": 16531.659016999998,
  "Queue time ms med": 0.042251,
  "Queue time ms max": 0.061671,
  "Queue time ms p(90)": 0.056599000000000003,
  "Queue time ms p(95)": 0.059135,
  "Queue time ms avg": 0.042577444444444446,
  "Queue time ms min": 0.026041
}