{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 18,
  "Virtual Users": 1,
  "Duration (s)": 314,
  "Throughput (tokens/second)": 28.614589288074836,
  "Latency (ms/token) avg": 33.07844677777778,
  "Latency (ms/token) min": 33.05428,
  "Latency (ms/token) med": 33.061951,
  "Latency (ms/token) max": 33.3392,
  "Latency (ms/token) p(90)": 33.0735626,
  "Latency (ms/token) p(95)": 33.1177053,
  "Latency Request ms p(90)": 16537.0230282,
  "Latency Request ms p(95)": 16559.065245499998,
  "Latency Request ms avg": 16539.537490388888,
  "Latency Request ms min": 16527.332506,
  "Latency Request ms med": 16531.172171,
  "Latency Request ms max": 16669.935157,
  "Latency Inference ms med": 16530.975864,
  "Latency Inference ms max": 16669.600482,
  "Latency Inference ms p(90)": 16536.7817015,
  "Latency Inference ms p(95)": 16558.85312605,
  "Latency Inference ms avg": 16539.2236385,
  "Latency Inference ms min": 16527.140213,
  "Queue time ms med": 0.0435655,
  "Queue time ms max": 0.05662,
  "Queue time ms p(90)": 0.0511403,
  "Queue time ms p(95)": 0.05547335,
  "Queue time ms avg": 0.04297222222222222,
  "Queue time ms min": 0.03318
}