{"Host": "sagemaker", "Model Id": "TheBloke/Llama-2-7B-GPTQ", "Instance": "ml.g5.2xlarge", "Tensor parallelism degree": 1, "quantization": "gptq", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 205, "Virtual Users": 5, "Throughput (tokens/second)": 120.09416333072869, "Latency (ms/token) avg": 37.703348697560976, "Latency (ms/token) min": 27.179833, "Latency (ms/token) med": 34.245736, "Latency (ms/token) max": 234.628068, "Latency (ms/token) p(90)": 44.3280604, "Latency (ms/token) p(95)": 54.1808366, "Latency Request ms p(90)": 3005.8895771999996, "Latency Request ms p(95)": 3808.7810157999998, "Latency Request ms avg": 2081.6998350829267, "Latency Request ms min": 147.068766, "Latency Request ms med": 1743.98413, "Latency Request ms max": 6332.543219, "Latency Inference ms med": 1706.940023, "Latency Inference ms max": 6331.591014, "Latency Inference ms p(90)": 2195.82242, "Latency Inference ms p(95)": 2673.093844999997, "Latency Inference ms avg": 1810.29363337561, "Latency Inference ms min": 121.073391, "Queue time ms med": 38.515149, "Queue time ms max": 4123.954118000001, "Queue time ms p(90)": 308.4296185999999, "Queue time ms p(95)": 1383.4555643999947, "Queue time ms avg": 270.3454486390244, "Queue time ms min": 0.02527}