{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 24,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 60.57605312176447,
  "Latency (ms/token) avg": 67.91653695833334,
  "Latency (ms/token) min": 59.1505,
  "Latency (ms/token) med": 68.94041949999999,
  "Latency (ms/token) max": 69.035327,
  "Latency (ms/token) p(90)": 68.9953168,
  "Latency (ms/token) p(95)": 69.01313585,
  "Latency Request ms p(90)": 57028.977415,
  "Latency Request ms p(95)": 58315.703116799996,
  "Latency Request ms avg": 41270.43396133333,
  "Latency Request ms min": 33003.460425,
  "Latency Request ms med": 35985.5711325,
  "Latency Request ms max": 59947.848947,
  "Latency Inference ms med": 34470.210160500006,
  "Latency Inference ms max": 34517.663712999994,
  "Latency Inference ms p(90)": 34497.658674800005,
  "Latency Inference ms p(95)": 34506.56830905,
  "Latency Inference ms avg": 33958.26876683333,
  "Latency Inference ms min": 29575.250076,
  "Queue time ms med": 1547.578154,
  "Queue time ms max": 25533.491034000002,
  "Queue time ms p(90)": 22524.3402095,
  "Queue time ms p(95)": 23808.736728899996,
  "Queue time ms avg": 7311.825782791667,
  "Queue time ms min": 0.048540999999999994
}