{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 23,
  "Virtual Users": 5,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 54.76180112936846,
  "Latency (ms/token) avg": 67.68564413043478,
  "Latency (ms/token) min": 59.390903,
  "Latency (ms/token) med": 68.696724,
  "Latency (ms/token) max": 69.024466,
  "Latency (ms/token) p(90)": 68.72900159999999,
  "Latency (ms/token) p(95)": 68.92960479999999,
  "Latency Request ms p(90)": 56122.785486600005,
  "Latency Request ms p(95)": 57120.6010334,
  "Latency Request ms avg": 40344.74870834782,
  "Latency Request ms min": 33055.792145,
  "Latency Request ms med": 35887.749991,
  "Latency Request ms max": 58619.042899,
  "Latency Inference ms med": 34348.362253,
  "Latency Inference ms max": 34512.23309,
  "Latency Inference ms p(90)": 34364.501184,
  "Latency Inference ms p(95)": 34464.802688100004,
  "Latency Inference ms avg": 33842.82235104348,
  "Latency Inference ms min": 29695.451985,
  "Queue time ms med": 1523.282761,
  "Queue time ms max": 24259.535339,
  "Queue time ms p(90)": 22699.159249999997,
  "Queue time ms p(95)": 22711.895506999997,
  "Queue time ms avg": 6501.607741521739,
  "Queue time ms min": 0.047022
}