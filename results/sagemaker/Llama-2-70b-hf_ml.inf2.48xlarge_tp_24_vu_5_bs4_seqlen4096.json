{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 36,
  "Virtual Users": 5,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 54.54531068434637,
  "Latency (ms/token) avg": 68.29443327777777,
  "Latency (ms/token) min": 59.293388,
  "Latency (ms/token) med": 68.9771265,
  "Latency (ms/token) max": 69.066054,
  "Latency (ms/token) p(90)": 69.02688599999999,
  "Latency (ms/token) p(95)": 69.03871425,
  "Latency Request ms p(90)": 56807.1535785,
  "Latency Request ms p(95)": 57306.8498005,
  "Latency Request ms avg": 41894.25602250001,
  "Latency Request ms min": 32975.104418,
  "Latency Request ms med": 36165.729498500004,
  "Latency Request ms max": 59932.665866,
  "Latency Inference ms med": 34488.56366,
  "Latency Inference ms max": 34533.027356,
  "Latency Inference ms p(90)": 34513.443224,
  "Latency Inference ms p(95)": 34519.357478499995,
  "Latency Inference ms avg": 34147.21687541666,
  "Latency Inference ms min": 29646.694248,
  "Queue time ms med": 1736.0426485,
  "Queue time ms max": 25457.582634000002,
  "Queue time ms p(90)": 22317.4029045,
  "Queue time ms p(95)": 22835.317348250002,
  "Queue time ms avg": 7746.715433861111,
  "Queue time ms min": 0.079551
}