{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 225,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 124.5863503396885,
  "Latency (ms/token) avg": 56.72556561333332,
  "Latency (ms/token) min": 29.866243,
  "Latency (ms/token) med": 54.070014,
  "Latency (ms/token) max": 66.985758,
  "Latency (ms/token) p(90)": 64.7534412,
  "Latency (ms/token) p(95)": 65.3753452,
  "Latency Request ms p(90)": 6456.2574956,
  "Latency Request ms p(95)": 6516.2990558,
  "Latency Request ms avg": 4013.2807377111108,
  "Latency Request ms min": 3170.351834,
  "Latency Request ms med": 3236.92161,
  "Latency Request ms max": 6632.322980999999,
  "Latency Inference ms med": 2703.500718,
  "Latency Inference ms max": 3349.28791,
  "Latency Inference ms p(90)": 3237.6720843999997,
  "Latency Inference ms p(95)": 3268.7672788,
  "Latency Inference ms avg": 2836.2783049955556,
  "Latency Inference ms min": 1493.312151,
  "Queue time ms med": 531.834847,
  "Queue time ms max": 3319.768621,
  "Queue time ms p(90)": 3220.2502396,
  "Queue time ms p(95)": 3247.4477658,
  "Queue time ms avg": 1176.8096259733334,
  "Queue time ms min": 0.047680999999999994
}