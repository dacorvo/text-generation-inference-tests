{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.p4d.24xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 862,
  "Virtual Users": 20,
  "Throughput (tokens/second)": 496.1627136046404,
  "Latency (ms/token) avg": 41.72628293967518,
  "Latency (ms/token) min": 24.333754,
  "Latency (ms/token) med": 38.696102,
  "Latency (ms/token) max": 434.892861,
  "Latency (ms/token) p(90)": 44.5523546,
  "Latency (ms/token) p(95)": 45.332098300000006,
  "Latency Request ms p(90)": 2330.0624133999995,
  "Latency Request ms p(95)": 2388.66788305,
  "Latency Request ms avg": 2015.4678547587005,
  "Latency Request ms min": 298.809841,
  "Latency Request ms med": 2022.9565465,
  "Latency Request ms max": 3056.377639,
  "Latency Inference ms med": 1927.436414,
  "Latency Inference ms max": 2325.814705,
  "Latency Inference ms p(90)": 2214.7598703,
  "Latency Inference ms p(95)": 2239.7394833999997,
  "Latency Inference ms avg": 1909.3496335,
  "Latency Inference ms min": 224.344521,
  "Queue time ms med": 80.65031400000001,
  "Queue time ms max": 1107.380795,
  "Queue time ms p(90)": 131.22871289999998,
  "Queue time ms p(95)": 226.54705324999986,
  "Queue time ms avg": 104.85983063573086,
  "Queue time ms min": 0.040279
}