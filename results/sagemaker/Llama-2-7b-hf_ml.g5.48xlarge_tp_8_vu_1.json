{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.g5.48xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 94,
  "Virtual Users": 1,
  "Duration (s)": 90,
  "Throughput (tokens/second)": 56.501382414569015,
  "Latency (ms/token) avg": 20.25150031914893,
  "Latency (ms/token) min": 15.489239,
  "Latency (ms/token) med": 17.9153435,
  "Latency (ms/token) max": 103.802619,
  "Latency (ms/token) p(90)": 22.668535700000003,
  "Latency (ms/token) p(95)": 24.105837049999995,
  "Latency Request ms p(90)": 1070.4818519,
  "Latency Request ms p(95)": 1126.6616414499997,
  "Latency Request ms avg": 884.9341000744679,
  "Latency Request ms min": 85.102796,
  "Latency Request ms med": 876.117482,
  "Latency Request ms max": 1186.1273139999998,
  "Latency Inference ms med": 875.5146345,
  "Latency Inference ms max": 1183.075946,
  "Latency Inference ms p(90)": 1068.2384379999999,
  "Latency Inference ms p(95)": 1123.8541073,
  "Latency Inference ms avg": 883.9317430638299,
  "Latency Inference ms min": 84.553289,
  "Queue time ms med": 0.039320499999999994,
  "Queue time ms max": 0.081151,
  "Queue time ms p(90)": 0.05846200000000001,
  "Queue time ms p(95)": 0.06719950000000001,
  "Queue time ms avg": 0.04122551063829787,
  "Queue time ms min": 0.02595
}
