{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-7b-hf", "Instance": "ml.g5.12xlarge", "Tensor parallelism degree": 4, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 603, "Virtual Users": 10, "Duration (s)": 330, "Throughput (tokens/second)": 348.52675073938207, "Latency (ms/token) avg": 31.177190459369818, "Latency (ms/token) min": 18.746898, "Latency (ms/token) med": 27.773879, "Latency (ms/token) max": 538.07994, "Latency (ms/token) p(90)": 35.520013, "Latency (ms/token) p(95)": 38.2811531, "Latency Request ms p(90)": 1819.9961136000002, "Latency Request ms p(95)": 1903.6550541, "Latency Request ms avg": 1434.610109379768, "Latency Request ms min": 136.095619, "Latency Request ms med": 1404.984471, "Latency Request ms max": 2189.0558659999997, "Latency Inference ms med": 1359.040227, "Latency Inference ms max": 2144.761544, "Latency Inference ms p(90)": 1699.0536318000002, "Latency Inference ms p(95)": 1812.4175159, "Latency Inference ms avg": 1376.1042462089551, "Latency Inference ms min": 82.679112, "Queue time ms med": 52.790367, "Queue time ms max": 734.06366, "Queue time ms p(90)": 70.5002408, "Queue time ms p(95)": 213.01798319999992, "Queue time ms avg": 57.59447551077944, "Queue time ms min": 0.046402}
