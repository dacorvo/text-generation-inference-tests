{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 23,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 60.29247761899741,
  "Latency (ms/token) avg": 70.00912860869565,
  "Latency (ms/token) min": 64.983909,
  "Latency (ms/token) med": 70.590416,
  "Latency (ms/token) max": 70.712867,
  "Latency (ms/token) p(90)": 70.66622720000001,
  "Latency (ms/token) p(95)": 70.6720772,
  "Latency Request ms p(90)": 57677.4679768,
  "Latency Request ms p(95)": 58321.6231669,
  "Latency Request ms avg": 41464.54248900001,
  "Latency Request ms min": 34518.525405,
  "Latency Request ms med": 36199.812571,
  "Latency Request ms max": 59183.140211,
  "Latency Inference ms med": 35295.208488,
  "Latency Inference ms max": 35356.433885,
  "Latency Inference ms p(90)": 35333.113714399995,
  "Latency Inference ms p(95)": 35336.0387699,
  "Latency Inference ms avg": 35004.56454565218,
  "Latency Inference ms min": 32491.954546,
  "Queue time ms med": 907.845631,
  "Queue time ms max": 23897.461363000002,
  "Queue time ms p(90)": 22920.982682,
  "Queue time ms p(95)": 22968.443089099997,
  "Queue time ms avg": 6459.664641652173,
  "Queue time ms min": 0.047729999999999995
}