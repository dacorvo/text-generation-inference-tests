{
  "Host": "sagemaker",
  "Model Id": "NousResearch/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 48,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 29.463304189422328,
  "Latency (ms/token) avg": 33.934542541666666,
  "Latency (ms/token) min": 33.873886,
  "Latency (ms/token) med": 33.939623999999995,
  "Latency (ms/token) max": 34.02565,
  "Latency (ms/token) p(90)": 33.9749937,
  "Latency (ms/token) p(95)": 33.9877338,
  "Latency Request ms p(90)": 1699.215524,
  "Latency Request ms p(95)": 1699.59593855,
  "Latency Request ms avg": 1697.0262289166667,
  "Latency Request ms min": 1693.9370119999999,
  "Latency Request ms med": 1697.2022124999999,
  "Latency Request ms max": 1701.486108,
  "Latency Inference ms med": 1696.9812345,
  "Latency Inference ms max": 1701.282525,
  "Latency Inference ms p(90)": 1698.7496999,
  "Latency Inference ms p(95)": 1699.3867030499998,
  "Latency Inference ms avg": 1696.7271515833334,
  "Latency Inference ms min": 1693.694349,
  "Queue time ms med": 0.041040499999999994,
  "Queue time ms max": 0.051841,
  "Queue time ms p(90)": 0.046460299999999996,
  "Queue time ms p(95)": 0.04736285,
  "Queue time ms avg": 0.04073602083333333,
  "Queue time ms min": 0.030011
}