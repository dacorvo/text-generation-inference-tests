{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 193,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 109.1109140329329,
  "Latency (ms/token) avg": 9.15687254404145,
  "Latency (ms/token) min": 8.31267,
  "Latency (ms/token) med": 9.160535,
  "Latency (ms/token) max": 9.936614,
  "Latency (ms/token) p(90)": 9.5563012,
  "Latency (ms/token) p(95)": 9.645465399999999,
  "Latency Request ms p(90)": 478.151487,
  "Latency Request ms p(95)": 482.66688519999997,
  "Latency Request ms avg": 458.24930020207256,
  "Latency Request ms min": 415.879667,
  "Latency Request ms med": 458.325518,
  "Latency Request ms max": 497.204115,
  "Latency Inference ms med": 458.02676,
  "Latency Inference ms max": 496.830715,
  "Latency Inference ms p(90)": 477.8150786,
  "Latency Inference ms p(95)": 482.27327799999995,
  "Latency Inference ms avg": 457.8436507979275,
  "Latency Inference ms min": 415.633514,
  "Queue time ms med": 0.019001,
  "Queue time ms max": 0.114966,
  "Queue time ms p(90)": 0.0277358,
  "Queue time ms p(95)": 0.0335114,
  "Queue time ms avg": 0.021080979274611396,
  "Queue time ms min": 0.01284
}