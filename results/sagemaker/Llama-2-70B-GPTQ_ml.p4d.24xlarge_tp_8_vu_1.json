{
  "Host": "sagemaker",
  "Model Id": "TheBloke/Llama-2-70B-GPTQ",
  "Instance": "ml.p4d.24xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "gptq",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 21,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 12.443744622405099,
  "Latency (ms/token) avg": 127.26652052380953,
  "Latency (ms/token) min": 62.861489,
  "Latency (ms/token) med": 67.990963,
  "Latency (ms/token) max": 1026.358532,
  "Latency (ms/token) p(90)": 170.171392,
  "Latency (ms/token) p(95)": 178.088255,
  "Latency Request ms p(90)": 5022.7602640000005,
  "Latency Request ms p(95)": 8508.896909000001,
  "Latency Request ms avg": 4018.083102571428,
  "Latency Request ms min": 2053.5152559999997,
  "Latency Request ms med": 3366.98422,
  "Latency Request ms max": 8905.026365,
  "Latency Inference ms med": 3366.6187710000004,
  "Latency Inference ms max": 8904.412777,
  "Latency Inference ms p(90)": 5022.194493,
  "Latency Inference ms p(95)": 8508.569606,
  "Latency Inference ms avg": 4017.363691619048,
  "Latency Inference ms min": 2052.717064,
  "Queue time ms med": 0.03836,
  "Queue time ms max": 0.078918,
  "Queue time ms p(90)": 0.06174,
  "Queue time ms p(95)": 0.062971,
  "Queue time ms avg": 0.04362209523809524,
  "Queue time ms min": 0.032784
}