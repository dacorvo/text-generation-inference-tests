{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 22,
  "Virtual Users": 5,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 52.38078994163753,
  "Latency (ms/token) avg": 69.3655065,
  "Latency (ms/token) min": 68.055859,
  "Latency (ms/token) med": 69.7405095,
  "Latency (ms/token) max": 69.78948,
  "Latency (ms/token) p(90)": 69.77262160000001,
  "Latency (ms/token) p(95)": 69.77267280000001,
  "Latency Request ms p(90)": 56944.250549700024,
  "Latency Request ms p(95)": 58262.2275239,
  "Latency Request ms avg": 40301.60178036364,
  "Latency Request ms min": 34087.576811,
  "Latency Request ms med": 35764.708192000006,
  "Latency Request ms max": 59157.525764,
  "Latency Inference ms med": 34870.2550855,
  "Latency Inference ms max": 34894.740375,
  "Latency Inference ms p(90)": 34886.31111460001,
  "Latency Inference ms p(95)": 34886.3365169,
  "Latency Inference ms avg": 34682.7535529091,
  "Latency Inference ms min": 34027.929955,
  "Queue time ms med": 892.9969289999999,
  "Queue time ms max": 24297.910161,
  "Queue time ms p(90)": 22066.527626800027,
  "Queue time ms p(95)": 23387.551172449996,
  "Queue time ms avg": 5618.479991363637,
  "Queue time ms min": 0.042793
}