{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 35,
  "Virtual Users": 5,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 53.03022246694976,
  "Latency (ms/token) avg": 69.57015962857142,
  "Latency (ms/token) min": 64.252189,
  "Latency (ms/token) med": 69.968975,
  "Latency (ms/token) max": 70.039002,
  "Latency (ms/token) p(90)": 70.0174766,
  "Latency (ms/token) p(95)": 70.0271228,
  "Latency Request ms p(90)": 58056.6206282,
  "Latency Request ms p(95)": 58138.0596867,
  "Latency Request ms avg": 41997.97274557142,
  "Latency Request ms min": 34167.443847999995,
  "Latency Request ms med": 36035.541326,
  "Latency Request ms max": 59132.003748999996,
  "Latency Inference ms med": 34984.487674,
  "Latency Inference ms max": 35019.501204,
  "Latency Inference ms p(90)": 35008.7386636,
  "Latency Inference ms p(95)": 35013.56156,
  "Latency Inference ms avg": 34785.080088828574,
  "Latency Inference ms min": 32126.094809000002,
  "Queue time ms med": 1111.528163,
  "Queue time ms max": 24130.128696,
  "Queue time ms p(90)": 23057.951612,
  "Queue time ms p(95)": 23153.8451696,
  "Queue time ms avg": 7212.5398152,
  "Queue time ms min": 0.049040999999999994
}