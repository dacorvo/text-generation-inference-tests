{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 23,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 61.01760021979377,
  "Latency (ms/token) avg": 69.18713760869565,
  "Latency (ms/token) min": 64.204038,
  "Latency (ms/token) med": 69.782596,
  "Latency (ms/token) max": 69.841057,
  "Latency (ms/token) p(90)": 69.83324859999999,
  "Latency (ms/token) p(95)": 69.8332889,
  "Latency Request ms p(90)": 57661.5921984,
  "Latency Request ms p(95)": 58268.4106079,
  "Latency Request ms avg": 40971.78504226087,
  "Latency Request ms min": 34110.298536999995,
  "Latency Request ms med": 35804.354246,
  "Latency Request ms max": 59208.601362,
  "Latency Inference ms med": 34891.29809,
  "Latency Inference ms max": 34920.528758,
  "Latency Inference ms p(90)": 34916.624454799996,
  "Latency Inference ms p(95)": 34916.644585,
  "Latency Inference ms avg": 34593.569015913046,
  "Latency Inference ms min": 32102.019292,
  "Queue time ms med": 893.491099,
  "Queue time ms max": 24323.839249,
  "Queue time ms p(90)": 23321.4409916,
  "Queue time ms p(95)": 23384.3063216,
  "Queue time ms avg": 6378.023070347828,
  "Queue time ms min": 0.03766
}