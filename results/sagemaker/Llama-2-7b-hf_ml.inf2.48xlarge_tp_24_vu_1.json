{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 41,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 112.14755557694441,
  "Latency (ms/token) avg": 8.915855219512196,
  "Latency (ms/token) min": 8.031648,
  "Latency (ms/token) med": 8.883738,
  "Latency (ms/token) max": 10.320108,
  "Latency (ms/token) p(90)": 9.893093,
  "Latency (ms/token) p(95)": 10.11232,
  "Latency Request ms p(90)": 4947.327324,
  "Latency Request ms p(95)": 5056.69655,
  "Latency Request ms avg": 4458.411932634146,
  "Latency Request ms min": 4016.3449809999997,
  "Latency Request ms med": 4442.612487,
  "Latency Request ms max": 5160.529587,
  "Latency Inference ms med": 4441.869393,
  "Latency Inference ms max": 5160.054048,
  "Latency Inference ms p(90)": 4946.546909000001,
  "Latency Inference ms p(95)": 5056.16045,
  "Latency Inference ms avg": 4457.927887170732,
  "Latency Inference ms min": 4015.824192,
  "Queue time ms med": 0.0235,
  "Queue time ms max": 0.141013,
  "Queue time ms p(90)": 0.039791,
  "Queue time ms p(95)": 0.071562,
  "Queue time ms avg": 0.029717609756097565,
  "Queue time ms min": 0.01411
}