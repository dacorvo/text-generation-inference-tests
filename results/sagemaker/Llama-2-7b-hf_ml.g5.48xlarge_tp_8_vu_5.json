{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.g5.48xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 324,
  "Virtual Users": 5,
  "Duration (s)": 90,
  "Throughput (tokens/second)": 189.6363316924556,
  "Latency (ms/token) avg": 28.231818827160488,
  "Latency (ms/token) min": 17.51411,
  "Latency (ms/token) med": 25.407989999999998,
  "Latency (ms/token) max": 186.233814,
  "Latency (ms/token) p(90)": 33.6689206,
  "Latency (ms/token) p(95)": 37.11034164999998,
  "Latency Request ms p(90)": 1727.3139097,
  "Latency Request ms p(95)": 1847.1276104499993,
  "Latency Request ms avg": 1318.3127819907406,
  "Latency Request ms min": 166.801093,
  "Latency Request ms med": 1285.083188,
  "Latency Request ms max": 2650.5212349999997,
  "Latency Inference ms med": 1247.735533,
  "Latency Inference ms max": 2102.9964410000002,
  "Latency Inference ms p(90)": 1657.5048313999998,
  "Latency Inference ms p(95)": 1738.2378878499999,
  "Latency Inference ms avg": 1259.9937331820988,
  "Latency Inference ms min": 159.911012,
  "Queue time ms med": 54.067812,
  "Queue time ms max": 817.258431,
  "Queue time ms p(90)": 86.65946309999995,
  "Queue time ms p(95)": 183.20364654999997,
  "Queue time ms avg": 57.47217166666666,
  "Queue time ms min": 0.027490999999999998
}
