{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 228,
  "Virtual Users": 20,
  "Duration (s)": 319,
  "Throughput (tokens/second)": 356.89935752135796,
  "Latency (ms/token) avg": 41.54222968421053,
  "Latency (ms/token) min": 33.586134,
  "Latency (ms/token) med": 41.191625,
  "Latency (ms/token) max": 43.474282,
  "Latency (ms/token) p(90)": 43.2868692,
  "Latency (ms/token) p(95)": 43.3697836,
  "Latency Request ms p(90)": 43172.5053616,
  "Latency Request ms p(95)": 43249.10429035,
  "Latency Request ms avg": 26795.891363122806,
  "Latency Request ms min": 21366.153687,
  "Latency Request ms med": 21636.5746565,
  "Latency Request ms max": 43366.983113,
  "Latency Inference ms med": 20595.812864,
  "Latency Inference ms max": 21737.141288,
  "Latency Inference ms p(90)": 21643.4347623,
  "Latency Inference ms p(95)": 21684.891962499998,
  "Latency Inference ms avg": 20771.115096004385,
  "Latency Inference ms min": 16793.06741,
  "Queue time ms med": 1049.220269,
  "Queue time ms max": 21715.549881000003,
  "Queue time ms p(90)": 21613.253458599997,
  "Queue time ms p(95)": 21649.827453249996,
  "Queue time ms avg": 6024.60032932456,
  "Queue time ms min": 0.057911000000000004
}