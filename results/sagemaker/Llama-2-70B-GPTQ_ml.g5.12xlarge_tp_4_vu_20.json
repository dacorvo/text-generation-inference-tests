{
  "Host": "sagemaker",
  "Model Id": "TheBloke/Llama-2-70B-GPTQ",
  "Instance": "ml.g5.12xlarge",
  "Tensor parallelism degree": 4,
  "quantization": "gptq",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 105,
  "Virtual Users": 20,
  "Duration (s)": 90,
"Throughput (tokens/second)": 56.27288348786125,
  "Latency (ms/token) avg": 500.76549772380946,
  "Latency (ms/token) min": 176.730343,
  "Latency (ms/token) med": 341.661761,
  "Latency (ms/token) max": 5699.229413,
  "Latency (ms/token) p(90)": 397.5393872,
  "Latency (ms/token) p(95)": 881.067479399999,
  "Latency Request ms p(90)": 25806.5716428,
  "Latency Request ms p(95)": 25834.7698172,
  "Latency Request ms avg": 17770.5484066,
  "Latency Request ms min": 2298.8730309999996,
  "Latency Request ms med": 17117.373370999998,
  "Latency Request ms max": 25875.458015,
  "Latency Inference ms med": 17011.545680000003,
  "Latency Inference ms max": 25840.387077,
  "Latency Inference ms p(90)": 19876.478014,
  "Latency Inference ms p(95)": 19876.856571999997,
  "Latency Inference ms avg": 16165.838930923808,
  "Latency Inference ms min": 1221.968322,
  "Queue time ms med": 34.327946,
  "Queue time ms max": 9743.657779,
  "Queue time ms p(90)": 5929.789342600001,
  "Queue time ms p(95)": 5943.147742,
  "Queue time ms avg": 1603.3199684857148,
  "Queue time ms min": 0.07329000000000001
}