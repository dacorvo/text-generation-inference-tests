{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 8,
  "Virtual Users": 10,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 12.121178088470211,
  "Latency (ms/token) avg": 69.661992875,
  "Latency (ms/token) min": 68.547001,
  "Latency (ms/token) med": 70.3117695,
  "Latency (ms/token) max": 70.344205,
  "Latency (ms/token) p(90)": 70.3437024,
  "Latency (ms/token) p(95)": 70.3439537,
  "Latency Request ms p(90)": 60049.8954441,
  "Latency Request ms p(95)": 60050.78170255,
  "Latency Request ms avg": 47582.416343,
  "Latency Request ms min": 35099.549678999996,
  "Latency Request ms med": 47589.90284350001,
  "Latency Request ms max": 60051.667961,
  "Latency Inference ms med": 35155.885166500004,
  "Latency Inference ms max": 35172.102509,
  "Latency Inference ms p(90)": 35171.8514771,
  "Latency Inference ms p(95)": 35171.97699305,
  "Latency Inference ms avg": 34830.9966955,
  "Latency Inference ms min": 34273.500702,
  "Queue time ms med": 12863.627246,
  "Queue time ms max": 24878.126345999997,
  "Queue time ms p(90)": 24877.5110201,
  "Queue time ms p(95)": 24877.818683049998,
  "Queue time ms avg": 12751.045956875001,
  "Queue time ms min": 0.040951
}