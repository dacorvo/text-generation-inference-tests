{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 53,
  "Virtual Users": 1,
  "Duration (s)": 300,
  "Throughput (tokens/second)": 88.21277523371656,
  "Latency (ms/token) avg": 11.286258886792456,
  "Latency (ms/token) min": 10.068615,
  "Latency (ms/token) med": 11.286247,
  "Latency (ms/token) max": 12.00441,
  "Latency (ms/token) p(90)": 11.7179886,
  "Latency (ms/token) p(95)": 11.845825,
  "Latency Request ms p(90)": 5859.3728812,
  "Latency Request ms p(95)": 5923.373986799999,
  "Latency Request ms avg": 5643.555185981132,
  "Latency Request ms min": 5034.926134,
  "Latency Request ms med": 5643.748105000001,
  "Latency Request ms max": 6002.59263,
  "Latency Inference ms med": 5643.123874999999,
  "Latency Inference ms max": 6002.205322,
  "Latency Inference ms p(90)": 5858.994401399999,
  "Latency Inference ms p(95)": 5922.9127158,
  "Latency Inference ms avg": 5643.129682566038,
  "Latency Inference ms min": 5034.307785,
  "Queue time ms med": 0.025042000000000002,
  "Queue time ms max": 0.080964,
  "Queue time ms p(90)": 0.03430300000000001,
  "Queue time ms p(95)": 0.04047239999999999,
  "Queue time ms avg": 0.02740260377358491,
  "Queue time ms min": 0.01856
}