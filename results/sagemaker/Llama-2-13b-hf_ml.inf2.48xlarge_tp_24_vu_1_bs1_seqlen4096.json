{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 36,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 110.97993370019043,
  "Latency (ms/token) avg": 9.009987916666667,
  "Latency (ms/token) min": 8.851167,
  "Latency (ms/token) med": 8.992116,
  "Latency (ms/token) max": 9.42014,
  "Latency (ms/token) p(90)": 9.059997,
  "Latency (ms/token) p(95)": 9.08103925,
  "Latency Request ms p(90)": 4530.3232905,
  "Latency Request ms p(95)": 4540.88041625,
  "Latency Request ms avg": 4505.318964694444,
  "Latency Request ms min": 4425.850133,
  "Latency Request ms med": 4496.5190445,
  "Latency Request ms max": 4710.876229,
  "Latency Inference ms med": 4496.0582715,
  "Latency Inference ms max": 4710.070273,
  "Latency Inference ms p(90)": 4529.998858,
  "Latency Inference ms p(95)": 4540.51995125,
  "Latency Inference ms avg": 4504.994239722222,
  "Latency Inference ms min": 4425.58351,
  "Queue time ms med": 0.044024999999999995,
  "Queue time ms max": 0.087211,
  "Queue time ms p(90)": 0.060330999999999996,
  "Queue time ms p(95)": 0.0677575,
  "Queue time ms avg": 0.046061722222222226,
  "Queue time ms min": 0.02366
}