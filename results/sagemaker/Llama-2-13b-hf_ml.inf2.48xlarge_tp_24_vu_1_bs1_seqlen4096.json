{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 39,
  "Virtual Users": 1,
  "Duration (s)": 180,
  "Throughput (tokens/second)": 108.22921027495997,
  "Latency (ms/token) avg": 8.979016692307692,
  "Latency (ms/token) min": 8.931008,
  "Latency (ms/token) med": 8.962588,
  "Latency (ms/token) max": 9.454033,
  "Latency (ms/token) p(90)": 8.9972318,
  "Latency (ms/token) p(95)": 9.008039100000001,
  "Latency Request ms p(90)": 4498.792511799999,
  "Latency Request ms p(95)": 4504.287709599999,
  "Latency Request ms avg": 4489.752897615384,
  "Latency Request ms min": 4465.736481999999,
  "Latency Request ms med": 4481.512165,
  "Latency Request ms max": 4727.336915999999,
  "Latency Inference ms med": 4481.294304,
  "Latency Inference ms max": 4727.016971,
  "Latency Inference ms p(90)": 4498.6162810000005,
  "Latency Inference ms p(95)": 4504.019773200001,
  "Latency Inference ms avg": 4489.508611051282,
  "Latency Inference ms min": 4465.50444,
  "Queue time ms med": 0.031571999999999996,
  "Queue time ms max": 0.06703400000000001,
  "Queue time ms p(90)": 0.047184,
  "Queue time ms p(95)": 0.056474,
  "Queue time ms avg": 0.0349694358974359,
  "Queue time ms min": 0.019671
}