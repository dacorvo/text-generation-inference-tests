{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-13b-hf", "Instance": "ml.p4d.24xlarge", "Tensor parallelism degree": 8, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 79, "Virtual Users": 1, "Throughput (tokens/second)": 47.67119329836331, "Latency (ms/token) avg": 22.032148924050635, "Latency (ms/token) min": 20.848694, "Latency (ms/token) med": 21.630097, "Latency (ms/token) max": 28.410837, "Latency (ms/token) p(90)": 24.368402, "Latency (ms/token) p(95)": 24.7632424, "Latency Request ms p(90)": 1183.8568474, "Latency Request ms p(95)": 1224.0110563, "Latency Request ms avg": 1048.8514455063294, "Latency Request ms min": 46.708942, "Latency Request ms med": 1076.995908, "Latency Request ms max": 1344.726061, "Latency Inference ms med": 1075.033983, "Latency Inference ms max": 1342.5371049999999, "Latency Inference ms p(90)": 1183.4352032000002, "Latency Inference ms p(95)": 1221.6239657999997, "Latency Inference ms avg": 1047.7685838860757, "Latency Inference ms min": 45.705024, "Queue time ms med": 0.034989, "Queue time ms max": 0.103862, "Queue time ms p(90)": 0.0681772, "Queue time ms p(95)": 0.07641490000000001, "Queue time ms avg": 0.042701987341772156, "Queue time ms min": 0.029945}