{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 41,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 21.28502679570933,
  "Latency (ms/token) avg": 165.2305181707317,
  "Latency (ms/token) min": 91.224582,
  "Latency (ms/token) med": 160.020883,
  "Latency (ms/token) max": 201.295166,
  "Latency (ms/token) p(90)": 192.021478,
  "Latency (ms/token) p(95)": 196.259145,
  "Latency Request ms p(90)": 18880.403661,
  "Latency Request ms p(95)": 19380.909242,
  "Latency Request ms avg": 11745.345796341462,
  "Latency Request ms min": 9000.384175000001,
  "Latency Request ms med": 9639.88613,
  "Latency Request ms max": 19701.84723,
  "Latency Inference ms med": 8001.044179999999,
  "Latency Inference ms max": 10064.758334,
  "Latency Inference ms p(90)": 9601.073912,
  "Latency Inference ms p(95)": 9812.957258,
  "Latency Inference ms avg": 8261.525929804877,
  "Latency Inference ms min": 4561.229133000001,
  "Queue time ms med": 1634.416323,
  "Queue time ms max": 10045.738962,
  "Queue time ms p(90)": 9576.478205000001,
  "Queue time ms p(95)": 9779.591577,
  "Queue time ms avg": 3483.564674804878,
  "Queue time ms min": 0.047740000000000005
}