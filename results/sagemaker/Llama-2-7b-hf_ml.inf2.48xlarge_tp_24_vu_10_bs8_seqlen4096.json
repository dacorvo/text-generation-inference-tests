{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 194,
  "Virtual Users": 10,
  "Duration (s)": 314,
  "Throughput (tokens/second)": 308.13692672909,
  "Latency (ms/token) avg": 24.531049309278348,
  "Latency (ms/token) min": 20.686723,
  "Latency (ms/token) med": 24.323041500000002,
  "Latency (ms/token) max": 25.55583,
  "Latency (ms/token) p(90)": 25.4024761,
  "Latency (ms/token) p(95)": 25.4168011,
  "Latency Request ms p(90)": 25343.401810900003,
  "Latency Request ms p(95)": 25384.19888525,
  "Latency Request ms avg": 15771.671634603092,
  "Latency Request ms min": 12584.004929,
  "Latency Request ms med": 12679.052703000001,
  "Latency Request ms max": 25460.96483,
  "Latency Inference ms med": 12161.521089,
  "Latency Inference ms max": 12777.915114000001,
  "Latency Inference ms p(90)": 12701.2384356,
  "Latency Inference ms p(95)": 12708.4009368,
  "Latency Inference ms avg": 12265.524901871133,
  "Latency Inference ms min": 10343.361836,
  "Queue time ms med": 520.0440745000001,
  "Queue time ms max": 12760.468945999999,
  "Queue time ms p(90)": 12676.281927,
  "Queue time ms p(95)": 12687.5082686,
  "Queue time ms avg": 3505.984369195876,
  "Queue time ms min": 0.057531
}