{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 98,
  "Virtual Users": 10,
  "Duration (s)": 193,
  "Throughput (tokens/second)": 252.58831985713272,
  "Latency (ms/token) avg": 29.160844938775515,
  "Latency (ms/token) min": 26.029099,
  "Latency (ms/token) med": 29.086593999999998,
  "Latency (ms/token) max": 31.363508,
  "Latency (ms/token) p(90)": 30.487795499999997,
  "Latency (ms/token) p(95)": 31.1703042,
  "Latency Request ms p(90)": 30390.3769243,
  "Latency Request ms p(95)": 30695.5875596,
  "Latency Request ms avg": 18692.87632163265,
  "Latency Request ms min": 14177.207482,
  "Latency Request ms med": 15489.3465375,
  "Latency Request ms max": 31219.936475000002,
  "Latency Inference ms med": 14543.2971165,
  "Latency Inference ms max": 15681.754067,
  "Latency Inference ms p(90)": 15243.898119899999,
  "Latency Inference ms p(95)": 15585.152225799999,
  "Latency Inference ms avg": 14580.422720795916,
  "Latency Inference ms min": 13014.549796000001,
  "Queue time ms med": 625.357158,
  "Queue time ms max": 15634.585377000001,
  "Queue time ms p(90)": 15219.5277152,
  "Queue time ms p(95)": 15534.3002978,
  "Queue time ms avg": 4112.294553693879,
  "Queue time ms min": 0.046452
}