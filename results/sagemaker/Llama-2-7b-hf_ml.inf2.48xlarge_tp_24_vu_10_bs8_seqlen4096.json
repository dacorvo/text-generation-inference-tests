{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 122,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 314.1983135468855,
  "Latency (ms/token) avg": 24.797905270491803,
  "Latency (ms/token) min": 20.827392,
  "Latency (ms/token) med": 24.618160500000002,
  "Latency (ms/token) max": 26.102067,
  "Latency (ms/token) p(90)": 25.6889669,
  "Latency (ms/token) p(95)": 25.8138383,
  "Latency Request ms p(90)": 25666.245949499997,
  "Latency Request ms p(95)": 25797.39094355,
  "Latency Request ms avg": 15913.516350729511,
  "Latency Request ms min": 12597.150559,
  "Latency Request ms med": 12840.1547195,
  "Latency Request ms max": 25890.968297,
  "Latency Inference ms med": 12309.0805495,
  "Latency Inference ms max": 13051.033801000001,
  "Latency Inference ms p(90)": 12844.483511,
  "Latency Inference ms p(95)": 12906.919348450001,
  "Latency Inference ms avg": 12398.952907286886,
  "Latency Inference ms min": 10413.696254,
  "Queue time ms med": 515.084026,
  "Queue time ms max": 12996.886477,
  "Queue time ms p(90)": 12821.1497226,
  "Queue time ms p(95)": 12869.621215000001,
  "Queue time ms avg": 3514.3190033360656,
  "Queue time ms min": 0.04845
}