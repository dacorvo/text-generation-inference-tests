{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-13b-hf", "Instance": "ml.g5.12xlarge", "Tensor parallelism degree": 4, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 79, "Virtual Users": 1, "Duration (s)": 90,
"Throughput (tokens/second)": 47.157365672584625, "Latency (ms/token) avg": 23.473072797468355, "Latency (ms/token) min": 19.111001, "Latency (ms/token) med": 21.002715, "Latency (ms/token) max": 87.86053, "Latency (ms/token) p(90)": 26.3438796, "Latency (ms/token) p(95)": 29.03390299999999, "Latency Request ms p(90)": 1284.1992538, "Latency Request ms p(95)": 1317.1864874, "Latency Request ms avg": 1060.2797524177217, "Latency Request ms min": 105.045404, "Latency Request ms med": 1041.7509140000002, "Latency Request ms max": 1446.09177, "Latency Inference ms med": 1040.977464, "Latency Inference ms max": 1444.3198209999998, "Latency Inference ms p(90)": 1281.6249966, "Latency Inference ms p(95)": 1315.1756684999998, "Latency Inference ms avg": 1059.2855928101264, "Latency Inference ms min": 104.433918, "Queue time ms med": 0.046001, "Queue time ms max": 0.081901, "Queue time ms p(90)": 0.059113, "Queue time ms p(95)": 0.062341, "Queue time ms avg": 0.0485829240506329, "Queue time ms min": 0.036789999999999996}