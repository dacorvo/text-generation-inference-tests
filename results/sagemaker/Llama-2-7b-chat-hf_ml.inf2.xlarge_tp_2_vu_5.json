{
  "Host": "sagemaker",
  "Model Id": "NousResearch/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 57,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 30.997559055890772,
  "Latency (ms/token) avg": 116.9548847017544,
  "Latency (ms/token) min": 78.777275,
  "Latency (ms/token) med": 113.38719,
  "Latency (ms/token) max": 131.077483,
  "Latency (ms/token) p(90)": 130.4792988,
  "Latency (ms/token) p(95)": 130.6527132,
  "Latency Request ms p(90)": 13025.853862400001,
  "Latency Request ms p(95)": 13037.952806399999,
  "Latency Request ms avg": 8065.151180105262,
  "Latency Request ms min": 6487.459019,
  "Latency Request ms med": 6509.936523,
  "Latency Request ms max": 13061.192476,
  "Latency Inference ms med": 5669.359545,
  "Latency Inference ms max": 6553.874178,
  "Latency Inference ms p(90)": 6523.9649624,
  "Latency Inference ms p(95)": 6532.635670799999,
  "Latency Inference ms avg": 5847.744257894737,
  "Latency Inference ms min": 3938.863757,
  "Queue time ms med": 838.626521,
  "Queue time ms max": 6529.132093,
  "Queue time ms p(90)": 6505.3632498,
  "Queue time ms p(95)": 6511.6874926,
  "Queue time ms avg": 2217.039397947369,
  "Queue time ms min": 0.040893
}