{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 168,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 105.02640624239652,
  "Latency (ms/token) avg": 34.01624697023809,
  "Latency (ms/token) min": 18.606024,
  "Latency (ms/token) med": 32.251323,
  "Latency (ms/token) max": 53.87338,
  "Latency (ms/token) p(90)": 39.9912276,
  "Latency (ms/token) p(95)": 41.342901499999996,
  "Latency Request ms p(90)": 3731.0345754,
  "Latency Request ms p(95)": 3967.7246862000006,
  "Latency Request ms avg": 2380.3537504940477,
  "Latency Request ms min": 1743.464997,
  "Latency Request ms med": 1893.9151685000002,
  "Latency Request ms max": 5000.895623,
  "Latency Inference ms med": 1612.566186,
  "Latency Inference ms max": 2693.669006,
  "Latency Inference ms p(90)": 1999.5613983000003,
  "Latency Inference ms p(95)": 2067.14509265,
  "Latency Inference ms avg": 1700.812371875,
  "Latency Inference ms min": 930.301216,
  "Queue time ms med": 283.2746095,
  "Queue time ms max": 2682.699439,
  "Queue time ms p(90)": 1882.539635,
  "Queue time ms p(95)": 1964.1750079,
  "Queue time ms avg": 679.3454975476191,
  "Queue time ms min": 0.032831000000000006
}