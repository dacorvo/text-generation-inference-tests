{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 96,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 249.59635019446335,
  "Latency (ms/token) avg": 31.61491963541667,
  "Latency (ms/token) min": 30.815653,
  "Latency (ms/token) med": 31.283379,
  "Latency (ms/token) max": 32.870666,
  "Latency (ms/token) p(90)": 32.6627,
  "Latency (ms/token) p(95)": 32.72401275,
  "Latency Request ms p(90)": 32620.84186,
  "Latency Request ms p(95)": 32694.52325275,
  "Latency Request ms avg": 20032.344207375,
  "Latency Request ms min": 16090.679719,
  "Latency Request ms med": 16328.476501,
  "Latency Request ms max": 32749.940093,
  "Latency Inference ms med": 15641.6898985,
  "Latency Inference ms max": 16435.33323,
  "Latency Inference ms p(90)": 16331.3503015,
  "Latency Inference ms p(95)": 16362.006602499998,
  "Latency Inference ms avg": 15807.460066739584,
  "Latency Inference ms min": 15407.826852,
  "Queue time ms med": 684.8373065000001,
  "Queue time ms max": 16398.699897,
  "Queue time ms p(90)": 16303.899881000001,
  "Queue time ms p(95)": 16349.744511750001,
  "Queue time ms avg": 4224.7104230625,
  "Queue time ms min": 0.06235
}