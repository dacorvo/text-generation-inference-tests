{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-7b-hf", "Instance": "ml.g5.48xlarge", "Tensor parallelism degree": 8, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 427, "Virtual Users": 10, "Throughput (tokens/second)": 245.15171004135973, "Latency (ms/token) avg": 45.978140562060894, "Latency (ms/token) min": 19.826167, "Latency (ms/token) med": 39.428618, "Latency (ms/token) max": 683.097265, "Latency (ms/token) p(90)": 55.65824, "Latency (ms/token) p(95)": 62.4671308, "Latency Request ms p(90)": 2791.0653976000003, "Latency Request ms p(95)": 3038.0014047, "Latency Request ms avg": 2039.5533847822014, "Latency Request ms min": 147.172275, "Latency Request ms med": 1933.264239, "Latency Request ms max": 3804.462998, "Latency Inference ms med": 1883.176974, "Latency Inference ms max": 3795.751456, "Latency Inference ms p(90)": 2688.2536998000005, "Latency Inference ms p(95)": 2963.327875, "Latency Inference ms avg": 1987.0587487189694, "Latency Inference ms min": 136.693556, "Queue time ms med": 9.691945, "Queue time ms max": 739.155038, "Queue time ms p(90)": 92.53110840000018, "Queue time ms p(95)": 224.4037045999999, "Queue time ms avg": 51.6168538173302, "Queue time ms min": 0.03982}