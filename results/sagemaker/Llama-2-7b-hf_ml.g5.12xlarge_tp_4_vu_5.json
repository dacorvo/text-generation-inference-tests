{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.g5.12xlarge",
  "Tensor parallelism degree": 4,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 380,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 225.11549027996423,
  "Latency (ms/token) avg": 24.11522961842105,
  "Latency (ms/token) min": 16.730962,
  "Latency (ms/token) med": 21.8502975,
  "Latency (ms/token) max": 191.26215,
  "Latency (ms/token) p(90)": 26.9551848,
  "Latency (ms/token) p(95)": 28.1334981,
  "Latency Request ms p(90)": 1353.407313,
  "Latency Request ms p(95)": 1388.9386567500005,
  "Latency Request ms avg": 1110.5410813315789,
  "Latency Request ms min": 191.740145,
  "Latency Request ms med": 1107.3642935,
  "Latency Request ms max": 1764.239984,
  "Latency Inference ms med": 1075.733236,
  "Latency Inference ms max": 1619.6652940000001,
  "Latency Inference ms p(90)": 1332.0661167,
  "Latency Inference ms p(95)": 1364.3490231000003,
  "Latency Inference ms avg": 1078.699358844737,
  "Latency Inference ms min": 186.148079,
  "Queue time ms med": 7.5174485,
  "Queue time ms max": 721.661674,
  "Queue time ms p(90)": 60.18315540000003,
  "Queue time ms p(95)": 78.06361030000001,
  "Queue time ms avg": 31.024271621052634,
  "Queue time ms min": 0.041110999999999995
}