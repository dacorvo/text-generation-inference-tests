{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 10,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 9.523775892091555,
  "Latency (ms/token) avg": 65.54586125,
  "Latency (ms/token) min": 62.556442,
  "Latency (ms/token) med": 65.54587000000001,
  "Latency (ms/token) max": 68.535263,
  "Latency (ms/token) p(90)": 68.5352441,
  "Latency (ms/token) p(95)": 68.53525355000001,
  "Latency Request ms p(90)": 34268.010032599996,
  "Latency Request ms p(95)": 34268.0326193,
  "Latency Request ms avg": 34259.033122249995,
  "Latency Request ms min": 34239.019712,
  "Latency Request ms med": 34264.5287855,
  "Latency Request ms max": 34268.055206,
  "Latency Inference ms med": 32772.9351185,
  "Latency Inference ms max": 34267.631543999996,
  "Latency Inference ms p(90)": 34267.6221324,
  "Latency Inference ms p(95)": 34267.6268382,
  "Latency Inference ms avg": 32772.930771,
  "Latency Inference ms min": 31278.221303,
  "Queue time ms med": 1480.341252,
  "Queue time ms max": 2982.739991,
  "Queue time ms p(90)": 2976.108107,
  "Queue time ms p(95)": 2979.4240489999997,
  "Queue time ms avg": 1485.8673469999999,
  "Queue time ms min": 0.046893000000000004
}