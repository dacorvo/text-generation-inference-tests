{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 10,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 6.060584248085851,
  "Latency (ms/token) avg": 69.7112065,
  "Latency (ms/token) min": 68.950023,
  "Latency (ms/token) med": 68.9501485,
  "Latency (ms/token) max": 71.994506,
  "Latency (ms/token) p(90)": 71.0812169,
  "Latency (ms/token) p(95)": 71.53786145,
  "Latency Request ms p(90)": 35997.251418399996,
  "Latency Request ms p(95)": 35997.499895199995,
  "Latency Request ms avg": 35973.628077,
  "Latency Request ms min": 35932.651214000005,
  "Latency Request ms med": 35982.056361,
  "Latency Request ms max": 35997.748371999995,
  "Latency Inference ms med": 34475.0742795,
  "Latency Inference ms max": 35997.253176,
  "Latency Inference ms p(90)": 35540.6085867,
  "Latency Inference ms p(95)": 35768.930881349996,
  "Latency Inference ms avg": 34855.6033845,
  "Latency Inference ms min": 34475.011803,
  "Queue time ms med": 1474.886434,
  "Queue time ms max": 1520.63919,
  "Queue time ms p(90)": 1512.2294499,
  "Queue time ms p(95)": 1516.4343199500001,
  "Queue time ms avg": 1117.6151845,
  "Queue time ms min": 0.04868
}