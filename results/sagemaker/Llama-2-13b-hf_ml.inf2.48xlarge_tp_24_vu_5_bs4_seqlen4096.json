{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 73,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 191.04491655662034,
  "Latency (ms/token) avg": 20.41720910958904,
  "Latency (ms/token) min": 18.827692,
  "Latency (ms/token) med": 20.424427,
  "Latency (ms/token) max": 22.041056,
  "Latency (ms/token) p(90)": 21.230020399999997,
  "Latency (ms/token) p(95)": 21.647624199999996,
  "Latency Request ms p(90)": 21085.5266036,
  "Latency Request ms p(95)": 21371.8546984,
  "Latency Request ms avg": 13085.927880520549,
  "Latency Request ms min": 10016.517826,
  "Latency Request ms med": 10653.203006,
  "Latency Request ms max": 21953.871235000002,
  "Latency Inference ms med": 10212.213556,
  "Latency Inference ms max": 11020.528259,
  "Latency Inference ms p(90)": 10615.010425600001,
  "Latency Inference ms p(95)": 10823.812460799998,
  "Latency Inference ms avg": 10208.60482820548,
  "Latency Inference ms min": 9413.84607,
  "Queue time ms med": 389.734609,
  "Queue time ms max": 10995.396193999999,
  "Queue time ms p(90)": 10573.631477,
  "Queue time ms p(95)": 10782.930438799998,
  "Queue time ms avg": 2877.0244199452054,
  "Queue time ms min": 0.063294
}