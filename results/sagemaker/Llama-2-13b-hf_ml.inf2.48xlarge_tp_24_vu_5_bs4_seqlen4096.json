{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 113,
  "Virtual Users": 5,
  "Duration (s)": 309,
  "Throughput (tokens/second)": 182.49775478191674,
  "Latency (ms/token) avg": 20.81830271681416,
  "Latency (ms/token) min": 18.792776,
  "Latency (ms/token) med": 20.880767,
  "Latency (ms/token) max": 22.772797,
  "Latency (ms/token) p(90)": 21.7066194,
  "Latency (ms/token) p(95)": 21.8199418,
  "Latency Request ms p(90)": 21496.179451400003,
  "Latency Request ms p(95)": 21806.365858399997,
  "Latency Request ms avg": 13338.044884690265,
  "Latency Request ms min": 9914.240446000002,
  "Latency Request ms med": 10820.0689,
  "Latency Request ms max": 22165.698301,
  "Latency Inference ms med": 10440.383762,
  "Latency Inference ms max": 11386.398738,
  "Latency Inference ms p(90)": 10853.309991999999,
  "Latency Inference ms p(95)": 10909.9709444,
  "Latency Inference ms avg": 10409.15159645133,
  "Latency Inference ms min": 9396.388428,
  "Queue time ms med": 379.360612,
  "Queue time ms max": 11354.559078,
  "Queue time ms p(90)": 10766.5991328,
  "Queue time ms p(95)": 10882.724801800001,
  "Queue time ms avg": 2928.6154912477878,
  "Queue time ms min": 0.02221
}