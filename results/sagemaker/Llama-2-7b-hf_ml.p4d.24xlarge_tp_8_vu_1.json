{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-7b-hf", "Instance": "ml.p4d.24xlarge", "Tensor parallelism degree": 8, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 72, "Virtual Users": 1, "Duration (s)": 90,
"Throughput (tokens/second)": 42.713207003848275, "Latency (ms/token) avg": 25.771177527777777, "Latency (ms/token) min": 21.490959, "Latency (ms/token) med": 23.501151999999998, "Latency (ms/token) max": 147.988506, "Latency (ms/token) p(90)": 25.7836675, "Latency (ms/token) p(95)": 26.05430475, "Latency Request ms p(90)": 1269.3989617, "Latency Request ms p(95)": 1298.4701784, "Latency Request ms avg": 1170.598124263889, "Latency Request ms min": 297.449339, "Latency Request ms med": 1173.0455695, "Latency Request ms max": 1615.2262779999999, "Latency Inference ms med": 1171.761153, "Latency Inference ms max": 1614.616467, "Latency Inference ms p(90)": 1268.0856822, "Latency Inference ms p(95)": 1296.4905078000002, "Latency Inference ms avg": 1169.5733498888887, "Latency Inference ms min": 295.977012, "Queue time ms med": 0.037886500000000004, "Queue time ms max": 0.089617, "Queue time ms p(90)": 0.0571128, "Queue time ms p(95)": 0.06490265000000002, "Queue time ms avg": 0.04126773611111111, "Queue time ms min": 0.02536}