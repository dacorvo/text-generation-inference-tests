{
  "Host": "sagemaker",
  "Model Id": "TheBloke/Llama-2-70B-GPTQ",
  "Instance": "ml.p4d.24xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "gptq",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 84,
  "Virtual Users": 5,
  "Duration (s)": 90,
"Throughput (tokens/second)": 46.82444359588558,
  "Latency (ms/token) avg": 118.89105496428571,
  "Latency (ms/token) min": 68.664589,
  "Latency (ms/token) med": 86.2460385,
  "Latency (ms/token) max": 950.465553,
  "Latency (ms/token) p(90)": 185.71573,
  "Latency (ms/token) p(95)": 219.84437689999996,
  "Latency Request ms p(90)": 9025.867104500001,
  "Latency Request ms p(95)": 9512.742616,
  "Latency Request ms avg": 5339.091739297619,
  "Latency Request ms min": 472.149857,
  "Latency Request ms med": 4419.686662,
  "Latency Request ms max": 13111.398528,
  "Latency Inference ms med": 4263.7581875,
  "Latency Inference ms max": 13110.779289,
  "Latency Inference ms p(90)": 7140.824019199999,
  "Latency Inference ms p(95)": 9280.0156731,
  "Latency Inference ms avg": 4811.283858845238,
  "Latency Inference ms min": 427.974507,
  "Queue time ms med": 54.175562,
  "Queue time ms max": 5945.794497,
  "Queue time ms p(90)": 1432.146563100001,
  "Queue time ms p(95)": 1946.35610975,
  "Queue time ms avg": 526.8066322380952,
  "Queue time ms min": 0.04768
}