{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.g5.48xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 488,
  "Virtual Users": 20,
  "Throughput (tokens/second)": 292.9755698810733,
  "Latency (ms/token) avg": 83.42130458401638,
  "Latency (ms/token) min": 38.500951,
  "Latency (ms/token) med": 69.7023005,
  "Latency (ms/token) max": 1411.4377220000001,
  "Latency (ms/token) p(90)": 90.949571,
  "Latency (ms/token) p(95)": 94.6599886,
  "Latency Request ms p(90)": 4559.0523333,
  "Latency Request ms p(95)": 4756.03469795,
  "Latency Request ms avg": 3413.2538778094263,
  "Latency Request ms min": 499.704591,
  "Latency Request ms med": 3423.7251134999997,
  "Latency Request ms max": 6601.089093,
  "Latency Inference ms med": 3352.7126515,
  "Latency Inference ms max": 5049.030257,
  "Latency Inference ms p(90)": 4419.299941,
  "Latency Inference ms p(95)": 4728.7143956499995,
  "Latency Inference ms avg": 3301.4420081311473,
  "Latency Inference ms min": 429.07526,
  "Queue time ms med": 65.7524065,
  "Queue time ms max": 2896.423242,
  "Queue time ms p(90)": 86.3727813,
  "Queue time ms p(95)": 167.7810539,
  "Queue time ms avg": 110.79590095286885,
  "Queue time ms min": 0.062511
}