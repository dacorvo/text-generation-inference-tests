{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-chat-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 178,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 96.49156269502132,
  "Latency (ms/token) avg": 72.92176542696629,
  "Latency (ms/token) min": 39.27103,
  "Latency (ms/token) med": 69.7912625,
  "Latency (ms/token) max": 85.562028,
  "Latency (ms/token) p(90)": 83.9123355,
  "Latency (ms/token) p(95)": 84.0845297,
  "Latency Request ms p(90)": 8368.1804451,
  "Latency Request ms p(95)": 8388.5081879,
  "Latency Request ms avg": 5181.800211696629,
  "Latency Request ms min": 4095.538715,
  "Latency Request ms med": 4186.3736265,
  "Latency Request ms max": 8521.658543,
  "Latency Inference ms med": 3489.5631495,
  "Latency Inference ms max": 4278.101441,
  "Latency Inference ms p(90)": 4195.616822,
  "Latency Inference ms p(95)": 4204.2264907,
  "Latency Inference ms avg": 3646.0882969382023,
  "Latency Inference ms min": 1963.551537,
  "Queue time ms med": 698.573771,
  "Queue time ms max": 4266.6444630000005,
  "Queue time ms p(90)": 4176.1498846,
  "Queue time ms p(95)": 4186.9366573,
  "Queue time ms avg": 1535.4199864213483,
  "Queue time ms min": 0.058381
}