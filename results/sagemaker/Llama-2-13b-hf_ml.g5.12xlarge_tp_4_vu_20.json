{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-13b-hf", "Instance": "ml.g5.12xlarge", "Tensor parallelism degree": 4, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 492, "Virtual Users": 20, "Throughput (tokens/second)": 295.63780706725936, "Latency (ms/token) avg": 88.36973810772359, "Latency (ms/token) min": 35.584401, "Latency (ms/token) med": 67.4027465, "Latency (ms/token) max": 1417.404136, "Latency (ms/token) p(90)": 82.59212939999999, "Latency (ms/token) p(95)": 151.43972554999914, "Latency Request ms p(90)": 4257.5173152, "Latency Request ms p(95)": 4417.600937349999, "Latency Request ms avg": 3382.5173103536586, "Latency Request ms min": 498.7942, "Latency Request ms med": 3471.6094034999996, "Latency Request ms max": 6388.645655, "Latency Inference ms med": 3309.5681545, "Latency Inference ms max": 4412.611221, "Latency Inference ms p(90)": 4112.7733686, "Latency Inference ms p(95)": 4129.494034, "Latency Inference ms avg": 3171.521568426829, "Latency Inference ms min": 348.611126, "Queue time ms med": 144.06203649999998, "Queue time ms max": 2536.945296, "Queue time ms p(90)": 159.33530979999998, "Queue time ms p(95)": 465.3189811499999, "Queue time ms avg": 209.8819643821138, "Queue time ms min": 0.060251}