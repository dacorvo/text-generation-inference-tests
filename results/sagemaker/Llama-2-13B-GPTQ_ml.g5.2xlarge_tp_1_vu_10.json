{"Host": "sagemaker", "Model Id": "TheBloke/Llama-2-13B-GPTQ", "Instance": "ml.g5.2xlarge", "Tensor parallelism degree": 1, "quantization": "gptq", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 159, "Virtual Users": 10, "Throughput (tokens/second)": 88.89488949972399, "Latency (ms/token) avg": 121.36833921383646, "Latency (ms/token) min": 74.911321, "Latency (ms/token) med": 96.174005, "Latency (ms/token) max": 799.422397, "Latency (ms/token) p(90)": 172.8457394, "Latency (ms/token) p(95)": 180.41753529999957, "Latency Request ms p(90)": 7315.679391, "Latency Request ms p(95)": 13368.0119163, "Latency Request ms avg": 5624.620299477987, "Latency Request ms min": 518.01115, "Latency Request ms med": 5177.110273, "Latency Request ms max": 13410.954800000001, "Latency Inference ms med": 4771.564827, "Latency Inference ms max": 13410.510902, "Latency Inference ms p(90)": 6221.2144132, "Latency Inference ms p(95)": 8642.309245100001, "Latency Inference ms avg": 4895.7468145974835, "Latency Inference ms min": 217.56916, "Queue time ms med": 260.727123, "Queue time ms max": 4751.979969, "Queue time ms p(90)": 1963.0061716, "Queue time ms p(95)": 4723.5372675, "Queue time ms avg": 727.5844227924529, "Queue time ms min": 0.041991}