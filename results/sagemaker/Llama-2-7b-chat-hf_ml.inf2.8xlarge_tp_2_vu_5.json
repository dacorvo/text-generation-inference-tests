{
  "Host": "sagemaker",
  "Model Id": "NousResearch/Llama-2-7b-chat-hf",
  "Instance": "ml.inf2.8xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 61,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 32.2343906448251,
  "Latency (ms/token) avg": 113.02281254098361,
  "Latency (ms/token) min": 77.261541,
  "Latency (ms/token) med": 109.778906,
  "Latency (ms/token) max": 125.884181,
  "Latency (ms/token) p(90)": 125.341475,
  "Latency (ms/token) p(95)": 125.64787,
  "Latency Request ms p(90)": 12516.873341,
  "Latency Request ms p(95)": 12535.181509,
  "Latency Request ms avg": 7755.691824754099,
  "Latency Request ms min": 6224.9799379999995,
  "Latency Request ms med": 6266.917472,
  "Latency Request ms max": 12565.736270000001,
  "Latency Inference ms med": 5488.945321,
  "Latency Inference ms max": 6294.209088,
  "Latency Inference ms p(90)": 6267.073776,
  "Latency Inference ms p(95)": 6282.393545,
  "Latency Inference ms avg": 5651.14065404918,
  "Latency Inference ms min": 3863.077063,
  "Queue time ms med": 770.307524,
  "Queue time ms max": 6281.183428,
  "Queue time ms p(90)": 6251.402017,
  "Queue time ms p(95)": 6266.433143,
  "Queue time ms avg": 2104.3946091967214,
  "Queue time ms min": 0.044843
}