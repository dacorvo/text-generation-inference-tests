{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.g5.2xlarge",
  "Tensor parallelism degree": 1,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 559,
  "Virtual Users": 20,
  "Throughput (tokens/second)": 313.5327912405814,
  "Latency (ms/token) avg": 65.68577910912342,
  "Latency (ms/token) min": 46.569995,
  "Latency (ms/token) med": 60.860308,
  "Latency (ms/token) max": 533.195128,
  "Latency (ms/token) p(90)": 71.128072,
  "Latency (ms/token) p(95)": 73.0713053,
  "Latency Request ms p(90)": 3686.7584846,
  "Latency Request ms p(95)": 3766.559922,
  "Latency Request ms avg": 3189.45905480322,
  "Latency Request ms min": 387.546937,
  "Latency Request ms med": 3239.8767679999996,
  "Latency Request ms max": 4967.968354,
  "Latency Inference ms med": 3020.720932,
  "Latency Inference ms max": 3710.968027,
  "Latency Inference ms p(90)": 3500.2076663999997,
  "Latency Inference ms p(95)": 3601.0597061000003,
  "Latency Inference ms avg": 2981.714252313059,
  "Latency Inference ms min": 124.92305,
  "Queue time ms med": 208.066466,
  "Queue time ms max": 1639.050819,
  "Queue time ms p(90)": 228.9111084,
  "Queue time ms p(95)": 470.9018690000001,
  "Queue time ms avg": 206.09614901073346,
  "Queue time ms min": 0.040311
}