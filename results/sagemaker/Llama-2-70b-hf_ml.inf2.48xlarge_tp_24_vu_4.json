{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 24,
  "Virtual Users": 4,
  "Throughput (tokens/second)": 58.332077198463345,
  "Latency (ms/token) avg": 66.34918370833333,
  "Latency (ms/token) min": 65.500673,
  "Latency (ms/token) med": 65.633715,
  "Latency (ms/token) max": 68.805359,
  "Latency (ms/token) p(90)": 68.5225126,
  "Latency (ms/token) p(95)": 68.6609496,
  "Latency Request ms p(90)": 34402.0930524,
  "Latency Request ms p(95)": 34402.823721650006,
  "Latency Request ms avg": 34286.45260129167,
  "Latency Request ms min": 34231.875752,
  "Latency Request ms med": 34255.564222,
  "Latency Request ms max": 34403.006104,
  "Latency Inference ms med": 32816.857887000006,
  "Latency Inference ms max": 34402.679647,
  "Latency Inference ms p(90)": 34261.2564153,
  "Latency Inference ms p(95)": 34330.47486725,
  "Latency Inference ms avg": 33174.59210558334,
  "Latency Inference ms min": 32750.336805000003,
  "Queue time ms med": 1481.7908775,
  "Queue time ms max": 1489.1253069999998,
  "Queue time ms p(90)": 1487.251299,
  "Queue time ms p(95)": 1488.71522885,
  "Queue time ms avg": 1111.6569739583333,
  "Queue time ms min": 0.018751
}