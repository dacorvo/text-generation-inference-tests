{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 104,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 295.6464312549107,
  "Latency (ms/token) avg": 13.264928000000001,
  "Latency (ms/token) min": 13.008108,
  "Latency (ms/token) med": 13.174563,
  "Latency (ms/token) max": 13.810239,
  "Latency (ms/token) p(90)": 13.684866,
  "Latency (ms/token) p(95)": 13.7273206,
  "Latency Request ms p(90)": 13655.7913594,
  "Latency Request ms p(95)": 13692.4069268,
  "Latency Request ms avg": 8456.046600624999,
  "Latency Request ms min": 6750.094336,
  "Latency Request ms med": 6837.3903365,
  "Latency Request ms max": 13768.959497999998,
  "Latency Inference ms med": 6587.2817350000005,
  "Latency Inference ms max": 6905.119865,
  "Latency Inference ms p(90)": 6842.4332809,
  "Latency Inference ms p(95)": 6863.66049435,
  "Latency Inference ms avg": 6632.464246653846,
  "Latency Inference ms min": 6504.0543179999995,
  "Queue time ms med": 249.177306,
  "Queue time ms max": 6880.7344920000005,
  "Queue time ms p(90)": 6825.1776956,
  "Queue time ms p(95)": 6844.5308706999995,
  "Queue time ms avg": 1823.3411059807693,
  "Queue time ms min": 0.052840000000000005
}