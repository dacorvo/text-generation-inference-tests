{"Host": "sagemaker", "Model Id": "TheBloke/Llama-2-13B-GPTQ", "Instance": "ml.g5.2xlarge", "Tensor parallelism degree": 1, "quantization": "gptq", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 45, "Virtual Users": 1, "Throughput (tokens/second)": 25.775191258143554, "Latency (ms/token) avg": 38.793540177777786, "Latency (ms/token) min": 28.321382, "Latency (ms/token) med": 31.054721, "Latency (ms/token) max": 127.479295, "Latency (ms/token) p(90)": 61.032052, "Latency (ms/token) p(95)": 66.75885579999998, "Latency Request ms p(90)": 3052.9340526000005, "Latency Request ms p(95)": 3339.9410435999994, "Latency Request ms avg": 1939.8498152444447, "Latency Request ms min": 1416.3268309999999, "Latency Request ms med": 1553.772817, "Latency Request ms max": 6374.529198, "Latency Inference ms med": 1552.736096, "Latency Inference ms max": 6373.964782, "Latency Inference ms p(90)": 3051.6026108, "Latency Inference ms p(95)": 3337.9428035999995, "Latency Inference ms avg": 1938.891076755556, "Latency Inference ms min": 1416.0691180000001, "Queue time ms med": 0.053231, "Queue time ms max": 0.080151, "Queue time ms p(90)": 0.0645566, "Queue time ms p(95)": 0.0662468, "Queue time ms avg": 0.05519735555555557, "Queue time ms min": 0.04431}