{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 20,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 9.523761897925239,
  "Latency (ms/token) avg": 66.44262875000001,
  "Latency (ms/token) min": 65.696869,
  "Latency (ms/token) med": 65.6969795,
  "Latency (ms/token) max": 68.679687,
  "Latency (ms/token) p(90)": 67.784886,
  "Latency (ms/token) p(95)": 68.2322865,
  "Latency Request ms p(90)": 34338.379554700005,
  "Latency Request ms p(95)": 34339.31049085,
  "Latency Request ms avg": 34301.6264385,
  "Latency Request ms min": 34265.708039000005,
  "Latency Request ms med": 34300.278143999996,
  "Latency Request ms max": 34340.241427,
  "Latency Inference ms med": 32848.490146,
  "Latency Inference ms max": 34339.843821,
  "Latency Inference ms p(90)": 33892.443324,
  "Latency Inference ms p(95)": 34116.143572500005,
  "Latency Inference ms avg": 33221.31473325,
  "Latency Inference ms min": 32848.43482,
  "Queue time ms med": 1417.4668025,
  "Queue time ms max": 1485.2128,
  "Queue time ms p(90)": 1465.0052704,
  "Queue time ms p(95)": 1475.1090351999999,
  "Queue time ms avg": 1080.057064,
  "Queue time ms min": 0.081851
}