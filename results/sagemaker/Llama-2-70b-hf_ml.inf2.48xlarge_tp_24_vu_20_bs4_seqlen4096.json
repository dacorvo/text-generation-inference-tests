{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 20,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 6.0605874458055675,
  "Latency (ms/token) avg": 66.5068995,
  "Latency (ms/token) min": 65.74597,
  "Latency (ms/token) med": 65.7461255,
  "Latency (ms/token) max": 68.789377,
  "Latency (ms/token) p(90)": 67.8764122,
  "Latency (ms/token) p(95)": 68.3328946,
  "Latency Request ms p(90)": 34391.8609968,
  "Latency Request ms p(95)": 34393.4376024,
  "Latency Request ms avg": 34365.4508125,
  "Latency Request ms min": 34308.706513,
  "Latency Request ms med": 34379.0412645,
  "Latency Request ms max": 34395.014208,
  "Latency Inference ms med": 32873.063021,
  "Latency Inference ms max": 34394.688598,
  "Latency Inference ms p(90)": 33938.2062022,
  "Latency Inference ms p(95)": 34166.4474001,
  "Latency Inference ms avg": 33253.449911749994,
  "Latency Inference ms min": 32872.985006999996,
  "Queue time ms med": 1467.902802,
  "Queue time ms max": 1511.1917779999999,
  "Queue time ms p(90)": 1507.9484204,
  "Queue time ms p(95)": 1509.5700992,
  "Queue time ms avg": 1111.7647539999998,
  "Queue time ms min": 0.061634
}