{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.g5.48xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 94,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 49.93779581743245,
  "Latency (ms/token) avg": 344.54468905319146,
  "Latency (ms/token) min": 122.051611,
  "Latency (ms/token) med": 206.77441750000003,
  "Latency (ms/token) max": 4306.19806,
  "Latency (ms/token) p(90)": 281.8246747,
  "Latency (ms/token) p(95)": 1303.634419449999,
  "Latency Request ms p(90)": 13973.1071942,
  "Latency Request ms p(95)": 14131.265121299999,
  "Latency Request ms avg": 10012.456333234042,
  "Latency Request ms min": 1492.499849,
  "Latency Request ms med": 9859.731737499998,
  "Latency Request ms max": 14667.568283999999,
  "Latency Inference ms med": 9821.156406,
  "Latency Inference ms max": 14091.272074,
  "Latency Inference ms p(90)": 13830.7608672,
  "Latency Inference ms p(95)": 14090.942610349999,
  "Latency Inference ms avg": 9773.517488893616,
  "Latency Inference ms min": 1250.249022,
  "Queue time ms med": 52.697314000000006,
  "Queue time ms max": 1772.452506,
  "Queue time ms p(90)": 858.4430197000001,
  "Queue time ms p(95)": 1470.5342065499972,
  "Queue time ms avg": 237.92936354255318,
  "Queue time ms min": 0.056763
}