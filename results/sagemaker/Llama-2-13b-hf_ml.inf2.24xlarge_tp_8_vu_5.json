{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.24xlarge",
  "Tensor parallelism degree": 8,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 41,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 108.50452760452995,
  "Latency (ms/token) avg": 36.17215741463414,
  "Latency (ms/token) min": 33.034689,
  "Latency (ms/token) med": 36.006905,
  "Latency (ms/token) max": 37.210092,
  "Latency (ms/token) p(90)": 37.173207,
  "Latency (ms/token) p(95)": 37.174112,
  "Latency Request ms p(90)": 37126.404179,
  "Latency Request ms p(95)": 37154.595137000004,
  "Latency Request ms avg": 23040.51319509756,
  "Latency Request ms min": 18513.164342,
  "Latency Request ms med": 18570.522925999998,
  "Latency Request ms max": 37169.046676,
  "Latency Inference ms med": 18003.452659000002,
  "Latency Inference ms max": 18605.046202,
  "Latency Inference ms p(90)": 18586.603982,
  "Latency Inference ms p(95)": 18587.056183,
  "Latency Inference ms avg": 18086.078965268294,
  "Latency Inference ms min": 16517.344986,
  "Queue time ms med": 565.190902,
  "Queue time ms max": 18581.896518,
  "Queue time ms p(90)": 18564.454880999998,
  "Queue time ms p(95)": 18567.90149,
  "Queue time ms avg": 4954.267630975611,
  "Queue time ms min": 0.024731000000000003
}
