{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 93,
  "Virtual Users": 5,
  "Duration (s)": 191,
  "Throughput (tokens/second)": 243.29088019870767,
  "Latency (ms/token) avg": 15.545176107526876,
  "Latency (ms/token) min": 12.842173,
  "Latency (ms/token) med": 15.583981,
  "Latency (ms/token) max": 17.291966,
  "Latency (ms/token) p(90)": 16.3335454,
  "Latency (ms/token) p(95)": 16.588092800000002,
  "Latency Request ms p(90)": 16106.170680399999,
  "Latency Request ms p(95)": 16397.5488722,
  "Latency Request ms avg": 9969.111113698926,
  "Latency Request ms min": 7684.564593,
  "Latency Request ms med": 8124.536392999999,
  "Latency Request ms max": 16798.310211,
  "Latency Inference ms med": 7791.9905309999995,
  "Latency Inference ms max": 8645.983197,
  "Latency Inference ms p(90)": 8166.7729988,
  "Latency Inference ms p(95)": 8294.046631000001,
  "Latency Inference ms avg": 7772.588321387097,
  "Latency Inference ms min": 6421.086645,
  "Queue time ms med": 308.611713,
  "Queue time ms max": 8612.545777,
  "Queue time ms p(90)": 8064.7002224,
  "Queue time ms p(95)": 8153.5766056,
  "Queue time ms avg": 2196.2157992150537,
  "Queue time ms min": 0.028901
}