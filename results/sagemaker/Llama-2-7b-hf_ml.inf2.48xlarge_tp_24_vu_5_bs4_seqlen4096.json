{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 105,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 286.73830136250524,
  "Latency (ms/token) avg": 13.613847657142859,
  "Latency (ms/token) min": 12.231896,
  "Latency (ms/token) med": 13.551572,
  "Latency (ms/token) max": 14.268829,
  "Latency (ms/token) p(90)": 14.0647034,
  "Latency (ms/token) p(95)": 14.1079182,
  "Latency Request ms p(90)": 14010.806492599999,
  "Latency Request ms p(95)": 14060.3797772,
  "Latency Request ms avg": 8718.75151704762,
  "Latency Request ms min": 6898.373523,
  "Latency Request ms med": 7021.369456,
  "Latency Request ms max": 14147.855415,
  "Latency Inference ms med": 6775.78602,
  "Latency Inference ms max": 7134.41496,
  "Latency Inference ms p(90)": 7032.3518884000005,
  "Latency Inference ms p(95)": 7053.959311600001,
  "Latency Inference ms avg": 6806.924079104761,
  "Latency Inference ms min": 6115.948082,
  "Queue time ms med": 246.270953,
  "Queue time ms max": 7112.896919,
  "Queue time ms p(90)": 7010.3133522,
  "Queue time ms p(95)": 7025.2403058,
  "Queue time ms avg": 1911.5936950857144,
  "Queue time ms min": 0.051421
}