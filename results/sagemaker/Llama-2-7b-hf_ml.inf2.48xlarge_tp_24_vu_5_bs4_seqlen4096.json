{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 144,
  "Virtual Users": 5,
  "Duration (s)": 314,
  "Throughput (tokens/second)": 229.2461171684962,
  "Latency (ms/token) avg": 16.179438104166668,
  "Latency (ms/token) min": 14.820303,
  "Latency (ms/token) med": 16.1228345,
  "Latency (ms/token) max": 18.526275,
  "Latency (ms/token) p(90)": 17.4542242,
  "Latency (ms/token) p(95)": 17.63484145,
  "Latency Request ms p(90)": 16863.9168274,
  "Latency Request ms p(95)": 17382.776656,
  "Latency Request ms avg": 10323.601701479165,
  "Latency Request ms min": 7679.713032000001,
  "Latency Request ms med": 8491.4712085,
  "Latency Request ms max": 18016.290806,
  "Latency Inference ms med": 8061.417316,
  "Latency Inference ms max": 9263.137870999999,
  "Latency Inference ms p(90)": 8727.1123837,
  "Latency Inference ms p(95)": 8817.4211377,
  "Latency Inference ms avg": 8089.719297562499,
  "Latency Inference ms min": 7410.151798,
  "Queue time ms med": 287.626848,
  "Queue time ms max": 9220.738217999999,
  "Queue time ms p(90)": 8394.3914472,
  "Queue time ms p(95)": 8691.03827505,
  "Queue time ms avg": 2233.5882614513885,
  "Queue time ms min": 0.06695999999999999
}