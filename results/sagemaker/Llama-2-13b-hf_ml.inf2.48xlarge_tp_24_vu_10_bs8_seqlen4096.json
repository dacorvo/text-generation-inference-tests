{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 82,
  "Virtual Users": 10,
  "Throughput (tokens/second)": 215.71876522453437,
  "Latency (ms/token) avg": 36.016573951219506,
  "Latency (ms/token) min": 30.571,
  "Latency (ms/token) med": 36.0343115,
  "Latency (ms/token) max": 39.43198,
  "Latency (ms/token) p(90)": 37.3672562,
  "Latency (ms/token) p(95)": 38.477990350000006,
  "Latency Request ms p(90)": 37586.7831323,
  "Latency Request ms p(95)": 37765.6847475,
  "Latency Request ms avg": 23178.3266272439,
  "Latency Request ms min": 17871.243082,
  "Latency Request ms med": 18949.66035,
  "Latency Request ms max": 38335.881311000005,
  "Latency Inference ms med": 18017.1560745,
  "Latency Inference ms max": 19715.990476000003,
  "Latency Inference ms p(90)": 18683.6283949,
  "Latency Inference ms p(95)": 19238.99527965,
  "Latency Inference ms avg": 18008.28722830488,
  "Latency Inference ms min": 15285.500237999999,
  "Queue time ms med": 933.4153885000001,
  "Queue time ms max": 19688.05232,
  "Queue time ms p(90)": 18648.736286299998,
  "Queue time ms p(95)": 19213.826833850002,
  "Queue time ms avg": 5169.838053975611,
  "Queue time ms min": 0.038312
}