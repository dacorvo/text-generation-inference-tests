{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 130,
  "Virtual Users": 10,
  "Duration (s)": 317,
  "Throughput (tokens/second)": 204.81210481470856,
  "Latency (ms/token) avg": 36.374044269230765,
  "Latency (ms/token) min": 30.202306,
  "Latency (ms/token) med": 36.06466399999999,
  "Latency (ms/token) max": 39.944398,
  "Latency (ms/token) p(90)": 37.951069200000006,
  "Latency (ms/token) p(95)": 38.42969395,
  "Latency Request ms p(90)": 38045.759606499996,
  "Latency Request ms p(95)": 38331.2811884,
  "Latency Request ms avg": 23438.047908946155,
  "Latency Request ms min": 17901.132828,
  "Latency Request ms med": 19088.36291,
  "Latency Request ms max": 38908.666812,
  "Latency Inference ms med": 18032.332195,
  "Latency Inference ms max": 19972.199334,
  "Latency Inference ms p(90)": 18975.534976900002,
  "Latency Inference ms p(95)": 19214.8472656,
  "Latency Inference ms avg": 18187.02237636923,
  "Latency Inference ms min": 15101.153352000001,
  "Queue time ms med": 886.6987935,
  "Queue time ms max": 19941.800738,
  "Queue time ms p(90)": 18999.7792331,
  "Queue time ms p(95)": 19181.65217175,
  "Queue time ms avg": 5250.81926963077,
  "Queue time ms min": 0.095621
}