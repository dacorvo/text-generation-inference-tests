{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 98,
  "Virtual Users": 10,
  "Duration (s)": 206,
  "Throughput (tokens/second)": 236.9644147434968,
  "Latency (ms/token) avg": 31.12628759183673,
  "Latency (ms/token) min": 26.633681,
  "Latency (ms/token) med": 30.874741,
  "Latency (ms/token) max": 32.46366,
  "Latency (ms/token) p(90)": 32.2373897,
  "Latency (ms/token) p(95)": 32.28691365,
  "Latency Request ms p(90)": 32184.6390253,
  "Latency Request ms p(95)": 32225.882440550005,
  "Latency Request ms avg": 19981.342745612244,
  "Latency Request ms min": 16008.702925000001,
  "Latency Request ms med": 16113.039754999998,
  "Latency Request ms max": 32390.371123,
  "Latency Inference ms med": 15437.370773999999,
  "Latency Inference ms max": 16231.830317,
  "Latency Inference ms p(90)": 16118.6951606,
  "Latency Inference ms p(95)": 16143.457280949999,
  "Latency Inference ms avg": 15563.144066316325,
  "Latency Inference ms min": 13316.840713,
  "Queue time ms med": 679.200745,
  "Queue time ms max": 16187.790893,
  "Queue time ms p(90)": 16097.0440984,
  "Queue time ms p(95)": 16140.87246165,
  "Queue time ms avg": 4418.012234081632,
  "Queue time ms min": 0.044171999999999996
}