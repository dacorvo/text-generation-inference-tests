{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 72,
  "Virtual Users": 5,
  "Throughput (tokens/second)": 225.0821977390944,
  "Latency (ms/token) avg": 17.495700333333332,
  "Latency (ms/token) min": 17.180034,
  "Latency (ms/token) med": 17.3391915,
  "Latency (ms/token) max": 18.262669,
  "Latency (ms/token) p(90)": 18.0028509,
  "Latency (ms/token) p(95)": 18.119744349999998,
  "Latency Request ms p(90)": 17965.0362574,
  "Latency Request ms p(95)": 18062.3920616,
  "Latency Request ms avg": 11107.053445861111,
  "Latency Request ms min": 8913.221528,
  "Latency Request ms med": 8986.983321,
  "Latency Request ms max": 18146.556967,
  "Latency Inference ms med": 8669.5962275,
  "Latency Inference ms max": 9131.334648,
  "Latency Inference ms p(90)": 9001.4257053,
  "Latency Inference ms p(95)": 9059.872377849999,
  "Latency Inference ms avg": 8747.850412749998,
  "Latency Inference ms min": 8590.017303,
  "Queue time ms med": 321.2675495,
  "Queue time ms max": 9099.452901999999,
  "Queue time ms p(90)": 8970.2718075,
  "Queue time ms p(95)": 9025.759954000001,
  "Queue time ms avg": 2358.96419675,
  "Queue time ms min": 0.057121000000000005
}