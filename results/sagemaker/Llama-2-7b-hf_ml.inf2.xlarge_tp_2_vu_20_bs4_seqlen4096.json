{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 20,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 9.523771914978447,
  "Latency (ms/token) avg": 69.86811625,
  "Latency (ms/token) min": 69.419283,
  "Latency (ms/token) med": 69.4193915,
  "Latency (ms/token) max": 71.214399,
  "Latency (ms/token) p(90)": 70.6759119,
  "Latency (ms/token) p(95)": 70.94515545,
  "Latency Request ms p(90)": 35599.5772585,
  "Latency Request ms p(95)": 35603.53310875,
  "Latency Request ms avg": 35568.208761249996,
  "Latency Request ms min": 35542.023109,
  "Latency Request ms med": 35561.6614885,
  "Latency Request ms max": 35607.488959,
  "Latency Inference ms med": 34709.6960085,
  "Latency Inference ms max": 35607.199905,
  "Latency Inference ms p(90)": 35337.9563007,
  "Latency Inference ms p(95)": 35472.57810285,
  "Latency Inference ms avg": 34934.058401250004,
  "Latency Inference ms min": 34709.641683,
  "Queue time ms med": 832.0415310000001,
  "Queue time ms max": 871.129915,
  "Queue time ms p(90)": 859.4241601,
  "Queue time ms p(95)": 865.2770375499999,
  "Queue time ms avg": 633.8125092500001,
  "Queue time ms min": 0.03706
}