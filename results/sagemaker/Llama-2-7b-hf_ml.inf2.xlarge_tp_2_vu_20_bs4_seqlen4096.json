{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.xlarge",
  "Tensor parallelism degree": 2,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 4,
  "Virtual Users": 20,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 6.0605988491362215,
  "Latency (ms/token) avg": 68.58549724999999,
  "Latency (ms/token) min": 68.135051,
  "Latency (ms/token) med": 68.1352045,
  "Latency (ms/token) max": 69.936529,
  "Latency (ms/token) p(90)": 69.39615939999999,
  "Latency (ms/token) p(95)": 69.6663442,
  "Latency Request ms p(90)": 34965.5527471,
  "Latency Request ms p(95)": 34967.05361155,
  "Latency Request ms avg": 34951.27965025,
  "Latency Request ms min": 34936.124283,
  "Latency Request ms med": 34950.219920999996,
  "Latency Request ms max": 34968.554476000005,
  "Latency Inference ms med": 34067.60247099999,
  "Latency Inference ms max": 34968.264671,
  "Latency Inference ms p(90)": 34698.07992049999,
  "Latency Inference ms p(95)": 34833.172295749995,
  "Latency Inference ms avg": 34292.74880424999,
  "Latency Inference ms min": 34067.525603999995,
  "Queue time ms med": 871.2072109999999,
  "Queue time ms max": 890.677323,
  "Queue time ms p(90)": 885.6500676,
  "Queue time ms p(95)": 888.1636953,
  "Queue time ms avg": 658.284709,
  "Queue time ms min": 0.047091
}