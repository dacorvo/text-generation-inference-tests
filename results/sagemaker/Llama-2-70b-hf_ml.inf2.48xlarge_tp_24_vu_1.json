{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-70b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 41,
  "Virtual Users": 1,
  "Throughput (tokens/second)": 22.35102953339657,
  "Latency (ms/token) avg": 44.73480558536586,
  "Latency (ms/token) min": 44.576872,
  "Latency (ms/token) med": 44.737821,
  "Latency (ms/token) max": 44.951228,
  "Latency (ms/token) p(90)": 44.806086,
  "Latency (ms/token) p(95)": 44.824836,
  "Latency Request ms p(90)": 2240.585094,
  "Latency Request ms p(95)": 2241.512433,
  "Latency Request ms avg": 2237.0334183170735,
  "Latency Request ms min": 2229.13432,
  "Latency Request ms med": 2237.250335,
  "Latency Request ms max": 2247.862588,
  "Latency Inference ms med": 2236.89109,
  "Latency Inference ms max": 2247.561423,
  "Latency Inference ms p(90)": 2240.3043000000002,
  "Latency Inference ms p(95)": 2241.2418000000002,
  "Latency Inference ms avg": 2236.740299414634,
  "Latency Inference ms min": 2228.843606,
  "Queue time ms med": 0.038011,
  "Queue time ms max": 0.076171,
  "Queue time ms p(90)": 0.056641,
  "Queue time ms p(95)": 0.05867,
  "Queue time ms avg": 0.04066324390243903,
  "Queue time ms min": 0.024041
}