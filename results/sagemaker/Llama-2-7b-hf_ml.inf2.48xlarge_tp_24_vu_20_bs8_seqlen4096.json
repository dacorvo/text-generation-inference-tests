{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 164,
  "Virtual Users": 20,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 248.4838932103575,
  "Latency (ms/token) avg": 30.414558475609756,
  "Latency (ms/token) min": 26.55493,
  "Latency (ms/token) med": 30.529541000000002,
  "Latency (ms/token) max": 32.53701,
  "Latency (ms/token) p(90)": 31.871501900000002,
  "Latency (ms/token) p(95)": 32.12805215,
  "Latency Request ms p(90)": 46759.9963327,
  "Latency Request ms p(95)": 46816.8434677,
  "Latency Request ms avg": 36777.33534157317,
  "Latency Request ms min": 14494.12424,
  "Latency Request ms med": 31843.393789499998,
  "Latency Request ms max": 47766.586685999995,
  "Latency Inference ms med": 15264.7706645,
  "Latency Inference ms max": 16268.505003000002,
  "Latency Inference ms p(90)": 15935.751207,
  "Latency Inference ms p(95)": 16064.026255499999,
  "Latency Inference ms avg": 15207.279504749999,
  "Latency Inference ms min": 13277.465382999999,
  "Queue time ms med": 16018.5288855,
  "Queue time ms max": 31866.179617,
  "Queue time ms p(90)": 31023.3677364,
  "Queue time ms p(95)": 31719.35960215,
  "Queue time ms avg": 21569.771165140246,
  "Queue time ms min": 0.06721099999999999
}