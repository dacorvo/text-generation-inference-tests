{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-7b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 132,
  "Virtual Users": 20,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 314.2839325711892,
  "Latency (ms/token) avg": 25.297940704545457,
  "Latency (ms/token) min": 22.198795,
  "Latency (ms/token) med": 25.439049,
  "Latency (ms/token) max": 25.634781,
  "Latency (ms/token) p(90)": 25.5896227,
  "Latency (ms/token) p(95)": 25.63426095,
  "Latency Request ms p(90)": 38238.3789521,
  "Latency Request ms p(95)": 38279.95300145,
  "Latency Request ms avg": 30389.705989242426,
  "Latency Request ms min": 12664.77185,
  "Latency Request ms med": 25538.581345,
  "Latency Request ms max": 38317.997482000006,
  "Latency Inference ms med": 12719.5247795,
  "Latency Inference ms max": 12817.390731,
  "Latency Inference ms p(90)": 12794.8117358,
  "Latency Inference ms p(95)": 12817.13057895,
  "Latency Inference ms avg": 12648.970583310605,
  "Latency Inference ms min": 11099.397892,
  "Queue time ms med": 12784.3594705,
  "Queue time ms max": 25545.274869,
  "Queue time ms p(90)": 25499.643574899997,
  "Queue time ms p(95)": 25537.8096844,
  "Queue time ms avg": 17740.598121234852,
  "Queue time ms min": 0.038350999999999996
}