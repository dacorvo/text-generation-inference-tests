{"Host": "sagemaker", "Model Id": "meta-llama/Llama-2-13b-hf", "Instance": "ml.g5.12xlarge", "Tensor parallelism degree": 4, "quantization": "none", "generated_tokens per request": 50, "Do Sample": true, "Number of requests": 278, "Virtual Users": 5, "Throughput (tokens/second)": 160.22998317259345, "Latency (ms/token) avg": 37.69732769784173, "Latency (ms/token) min": 22.17405, "Latency (ms/token) med": 30.9336655, "Latency (ms/token) max": 375.224389, "Latency (ms/token) p(90)": 40.281593300000004, "Latency (ms/token) p(95)": 44.135724899999914, "Latency Request ms p(90)": 1999.1585802, "Latency Request ms p(95)": 2062.9370515, "Latency Request ms avg": 1560.257294233813, "Latency Request ms min": 152.165557, "Latency Request ms med": 1566.0013325, "Latency Request ms max": 2374.453667, "Latency Inference ms med": 1496.5383674999998, "Latency Inference ms max": 2363.329329, "Latency Inference ms p(90)": 1955.5355681, "Latency Inference ms p(95)": 2017.6524267999996, "Latency Inference ms avg": 1500.1220480791367, "Latency Inference ms min": 141.208093, "Queue time ms med": 12.805405, "Queue time ms max": 602.338681, "Queue time ms p(90)": 183.7546946, "Queue time ms p(95)": 245.01575539999982, "Queue time ms avg": 59.26229302877698, "Queue time ms min": 0.043401}