{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 108,
  "Virtual Users": 20,
  "Duration (s)": 210,
  "Throughput (tokens/second)": 257.14130052452515,
  "Latency (ms/token) avg": 32.23913268518519,
  "Latency (ms/token) min": 28.661884,
  "Latency (ms/token) med": 32.4120375,
  "Latency (ms/token) max": 32.519692,
  "Latency (ms/token) p(90)": 32.4407157,
  "Latency (ms/token) p(95)": 32.51908155,
  "Latency Request ms p(90)": 48631.3605063,
  "Latency Request ms p(95)": 48635.68254075,
  "Latency Request ms avg": 38336.53123532407,
  "Latency Request ms min": 16205.324224000002,
  "Latency Request ms med": 32463.510462500002,
  "Latency Request ms max": 48683.795398999995,
  "Latency Inference ms med": 16206.0190695,
  "Latency Inference ms max": 16259.846137,
  "Latency Inference ms p(90)": 16220.3579893,
  "Latency Inference ms p(95)": 16259.54115965,
  "Latency Inference ms avg": 16119.56658987963,
  "Latency Inference ms min": 14330.942385,
  "Queue time ms med": 16206.835215000001,
  "Queue time ms max": 32466.434737999996,
  "Queue time ms p(90)": 32411.0933778,
  "Queue time ms p(95)": 32462.95611635,
  "Queue time ms avg": 22216.80801206481,
  "Queue time ms min": 0.08244199999999999
}