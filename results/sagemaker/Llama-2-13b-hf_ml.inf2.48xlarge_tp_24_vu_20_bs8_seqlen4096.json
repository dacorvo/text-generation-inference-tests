{
  "Host": "sagemaker",
  "Model Id": "meta-llama/Llama-2-13b-hf",
  "Instance": "ml.inf2.48xlarge",
  "Tensor parallelism degree": 24,
  "quantization": "none",
  "generated_tokens per request": 500,
  "Do Sample": true,
  "Number of requests": 160,
  "Virtual Users": 20,
  "Duration (s)": 330,
  "Throughput (tokens/second)": 242.4237748739963,
  "Latency (ms/token) avg": 32.16448573125,
  "Latency (ms/token) min": 31.754796,
  "Latency (ms/token) med": 32.210566,
  "Latency (ms/token) max": 32.440098,
  "Latency (ms/token) p(90)": 32.372244900000005,
  "Latency (ms/token) p(95)": 32.37431085,
  "Latency Request ms p(90)": 48316.8662859,
  "Latency Request ms p(95)": 48354.08829959999,
  "Latency Request ms avg": 38599.7859014625,
  "Latency Request ms min": 16162.169316,
  "Latency Request ms med": 32347.050684499998,
  "Latency Request ms max": 48534.098139,
  "Latency Inference ms med": 16105.283363499999,
  "Latency Inference ms max": 16220.049008,
  "Latency Inference ms p(90)": 16186.122905199998,
  "Latency Inference ms p(95)": 16187.155706100002,
  "Latency Inference ms avg": 16082.243134449998,
  "Latency Inference ms min": 15877.398308,
  "Queue time ms med": 16163.5690565,
  "Queue time ms max": 32346.441593000003,
  "Queue time ms p(90)": 32226.3192301,
  "Queue time ms p(95)": 32256.81637915,
  "Queue time ms avg": 22517.375770918752,
  "Queue time ms min": 0.035101
}