{
  "Host": "sagemaker",
  "Model Id": "TheBloke/Llama-2-70B-GPTQ",
  "Instance": "ml.g5.12xlarge",
  "Tensor parallelism degree": 4,
  "quantization": "gptq",
  "generated_tokens per request": 50,
  "Do Sample": true,
  "Number of requests": 26,
  "Virtual Users": 1,
  "Duration (s)": 90,
"Throughput (tokens/second)": 14.748987561461242,
  "Latency (ms/token) avg": 121.90383526923077,
  "Latency (ms/token) min": 53.61672,
  "Latency (ms/token) med": 57.2349955,
  "Latency (ms/token) max": 1020.5872529999999,
  "Latency (ms/token) p(90)": 163.75261849999998,
  "Latency (ms/token) p(95)": 376.27106449999997,
  "Latency Request ms p(90)": 4639.474583,
  "Latency Request ms p(95)": 7126.748658,
  "Latency Request ms avg": 3390.063202076923,
  "Latency Request ms min": 891.585751,
  "Latency Request ms med": 2779.1524725,
  "Latency Request ms max": 8490.115268,
  "Latency Inference ms med": 2778.6659345000003,
  "Latency Inference ms max": 8489.413155999999,
  "Latency Inference ms p(90)": 4638.2755885,
  "Latency Inference ms p(95)": 7126.1213185,
  "Latency Inference ms avg": 3389.310090961538,
  "Latency Inference ms min": 890.19733,
  "Queue time ms med": 0.0471405,
  "Queue time ms max": 0.070581,
  "Queue time ms p(90)": 0.062321,
  "Queue time ms p(95)": 0.0649685,
  "Queue time ms avg": 0.050349153846153856,
  "Queue time ms min": 0.041409999999999995
}